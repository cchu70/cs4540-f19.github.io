{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\vx}{\\vec{x}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# Lecture 17:  Naive Bayes Algorithm\n",
    "\n",
    "Naveen Kodali and Jacob Abernethy\n",
    "*Date:  Thursday, October 25, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "- Basics\n",
    "    - Bayes Rule\n",
    "    - MLE for binomial, multinomial distributions\n",
    "    - MLE for linear regression\n",
    "    - MAP estimation for linear regression\n",
    "- Naive Bayes Classifier\n",
    "    - Independence Assumption\n",
    "    - MLE Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Rule \n",
    "\n",
    "P(A|B) = P(B|A)P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.a\n",
    "\n",
    "You go to see the doctor about an ingrowing toenail. The doctor selects you at random to have\n",
    "a blood test for swine flu, which for the purposes of this exercise we will say is currently suspected\n",
    "to affect 1 in 10,000 people in Australia. The test is 99% accurate, in the sense that the probability\n",
    "of a false positive is 1%. The probability of a false negative is zero. You test positive. What is the\n",
    "new probability that you have swine flu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1.b\n",
    "\n",
    "Now imagine that you went to a friendâ€™s wedding in Mexico recently, and (for the purposes of this\n",
    "exercise) it is know that 1 in 200 people who visited Mexico recently come back with swine flu.\n",
    "Given the same test result as above, what should your revised estimate be for the probability you\n",
    "have the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML for binomial distribution (Exercise 2)\n",
    "\n",
    "\n",
    "\n",
    "Consider a series of N experiments with outcomes $\\{y_1,...y_N\\}$, where each $y_i$ is in $\\{0,1\\}$. The probability of this sequence is given by a binomial distrbution with some unknown parameter $p$ (the probability that $y_i = 1$). That is, if $k = y_1 + \\ldots + y_N$, then\n",
    "$$P(k | p) := \\binom{N}{k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "Given the data, what is the maximum likelihood estimate for $p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML for multinomial distribution (Exercise 3)\n",
    "\n",
    "Consider a series of N experiments with outcomes$\\{y_1,...y_N\\}$, where each $y_i$ is in $\\{1,2..,K\\}$. We know that the probability of this sequence is given by a multinomial distrbution with some unknown parameters $(p_1,..,p_K)$, i.e. the probability of $y_i = j$ is given by $p_j$. If we write $x_j$ to be the *count* of the number of times $y_i = j$, then probability distribution is given by\n",
    "$$P(x_1, \\ldots, x_K | p_1, \\ldots, p_K) := \\binom{n}{x_1, \\ldots, x_K} p_1^{x_1} \\cdots p_K^{x_K}$$\n",
    "\n",
    "\n",
    "Given the data, what are the maximum likelihood estimates for each of these p_i?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood: Simple Linear Regression\n",
    "\n",
    "One of the most basic techniques for trying to estimate a real number $y$ given an observed vector $x \\in \\R^d$ is known as *linear regression*, aka *least squares*. The key idea is to assume that the data $x_1, \\ldots, x_n$ are fixed vectors, and the labels $y_1, \\ldots, y_n$ are *independent* random real numbers whose probabiliy distribution is *guassian*. For some parameter $\\theta \\in \\R^d$ we have\n",
    "$$P( y_i | x_i, \\theta) = \\mathcal{N}(y_i | x_i^\\top \\theta, \\sigma^2)= \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^\\top \\theta)^2}{2\\sigma^2}\\right)$$\n",
    "What is the MLE for $\\theta$? (You don't necessarily have to find a closed-form solution)\n",
    "\n",
    "Useful fact: by independence we have $P(y_1, \\ldots, y_n | x_1, \\ldots, x_n, \\theta) = P(y_1 | x_1, \\theta) \\cdots P(y_n | x_n, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum A Posterior estimation\n",
    "\n",
    "Previously, we have been looking at the Max. Likelihood estimator. A more advanced estimator involves a *prior* on the parameters. This means that we \"prefer\", a priori, certain parameters over others. In other words, we think some parameters are more likely to be true than others. You describe your \"preference\" for some parameters using a prior distribution $P(\\theta)$. Given data $x_1, \\ldots, x_n$, the Maximum A Posterior (MAP) estimate for $\\theta$ is then\n",
    "$$\\arg\\max_\\theta P(\\theta) \\prod_{i=1}^n P(x_i | \\theta) \\quad \\text{ equiv. to } \\quad \n",
    "\\arg\\max_\\theta \\log P(\\theta) +  \\sum_{i=1}^n \\log P(x_i | \\theta) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "For the linear regression model above, we can add a prior to the parameter vector $\\theta$. The standard prior is the so-called \"multivariate gaussian\" with a single *fixed* parameter $\\nu$, i.e.\n",
    "$$P(\\theta) = \\frac{1}{(2\\pi \\nu^2)^{d/2}} \\exp\\left(\\frac{\\|\\theta\\|^2}{2\\nu^2}\\right)$$\n",
    "\n",
    "The likelihood function is still the same:\n",
    "$P( y_i | x_i, \\theta) =\n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^\\top \\theta)^2}{2\\sigma^2}\\right)$. \n",
    "\n",
    "**Problem**: What is the MAP estimate for this model?\n",
    "$$\\arg\\max_\\theta \\log P(\\theta) +  \\sum_{i=1}^n \\log P(y_i | x_i, \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Classification\n",
    "\n",
    "- Given some data X = {x1,...,xn} and their labels Y = {y1,...,yn} \\in {1,...,K}. The goal of classification is to find a function f: X -> Pr(Y=k|X) that fits this data and also performs well on unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Problem\n",
    "\n",
    "- We will use **Naive Bayes** to solve the following classification problem:\n",
    "    - **Categorical** feature vector $\\vx = (x_1, x_2, \\dots, x_D)$ with length $D$\n",
    "        - Each feature $x_d \\in \\{1, \\dots ,M \\}$, $\\forall d = 1, \\dots, D$\n",
    "    - Predict discrete class label $y \\in \\{1, 2, \\dots, C \\}$\n",
    "\n",
    "- For example, in **Spam Mail Classification**,\n",
    "    - Predict whether an email is `SPAM` ($y=1$) or `HAM` ($y=0$)\n",
    "    - Use words / metadata in the email as features\n",
    "    - For simplicity, we can use **bag-of-words** features,\n",
    "        - Assume fixed vocabulary $V$ of size $|V| = D$\n",
    "        - Feature $x_d$, for $d \\in \\{1, 2, \\dots, D \\}$, indicates the existence of $d\\text{th}$ word in the email\n",
    "        - Eg. $x_d = 1$ if $d\\text{th}$ word is in the email; $x_d = 0$ otherwise\n",
    "        - In this case $M=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Independence Assumption and Full model\n",
    "\n",
    "- The essence of Naive Bayes is the **conditionally independence assumption**\n",
    "    $$\n",
    "    P(\\vx | y = c) = \\prod_{d=1}^D P(x_d | y=c)\n",
    "    $$\n",
    "    i.e., given the label, all features are independent.\n",
    "    \n",
    "- The **full generative** model of Naive Bayes is:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y       &\\sim \\mathrm{Categorical}(\\pi) \\\\\n",
    "    x_d | y=c &\\sim \\mathrm{Categorical}(\\theta_{cd}) \\quad \\forall\\, d = 1,\\dots,D\n",
    "    \\end{align}\n",
    "    $$\n",
    "    with parameters:\n",
    "    - **Class priors** $\\pi = (\\pi_1, \\dots, \\pi_C) \\in \\Delta^C$, \n",
    "        - i.e. $P(y = c) = \\pi_c$, $\\forall c = 1,\\dots,C $\n",
    "        - $\\Delta^C$ is C-**simplex**. $\\pi \\in \\Delta^C$ is saying that $\\sum_{c=1}^C \\pi_c = 1$ and $\\pi_c \\geq 0, \\forall c=1,\\dots,C$\n",
    "    - **Class-conditional probabilities** $\\theta_{cd} = (\\theta_{cd1},  \\dots, \\theta_{cdM}) \\in \\Delta^M$\n",
    "        - i.e. $P(x_d = m| y = c) = \\theta_{cdm}$ for every $d = 1,\\dots,D, m = 1, \\dots, M, c = 1, \\dots, C$\n",
    "\n",
    "- Parameter $\\pi$ and $\\theta$ are learned from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ">**Remark**\n",
    "> - **NOTE** in definition and derivation of this lecture, we assume a more general case $x_d \\in \\{1, \\dots ,M \\}$ of which $M>2$. But in spam email classification and the derivation in textbook, binary feature, i.e. $M=2$, is used. So don't get confused!\n",
    "\n",
    "> - When $M=2$, $x_d | y=c$ is also Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Prediction\n",
    "\n",
    "- Given the independence assumption and full model, for some new data $\\vx^{\\text{new}} = (x_1^{\\text{new}}, \\dots, x_D^{\\text{new}})$ we will classify based on\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(y=c|\\vx = \\vx^{\\text{new}}) \\\\\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(\\vx = \\vx^{\\text{new}} | y=c) P(y=c) \\\\\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(y=c) \\prod \\nolimits_{d=1}^{D} P(x_d = x_d^{\\text{new}} | y=c) \\\\\n",
    "    &=\\boxed{\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\theta_{cdx_d^{\\text{new}}}} \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "- If we assume $x_d^{\\text{new}} \\in \\{1,\\dots,M \\}, \\forall d = 1,\\dots,D$, we could also express the above expression equivalently using **indicator function**\n",
    "    $$\n",
    "    y = \\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\prod \\nolimits_{m=1}^{M} \\theta_{cdm}^{\\mathbb{I}(m=x_d^{\\text{new}})}\n",
    "    $$\n",
    "    \n",
    "- So as long as we learned parameter $\\pi$ and $\\theta$, we could classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> **Remark**\n",
    "\n",
    "> - Indicator function\n",
    "    $$\n",
    "    \\mathbb{I}(m=x_d^{\\text{new}}) = \n",
    "    \\begin{cases}\n",
    "    1 & \\text{ if } m=x_d^{\\text{new}}\\\\ \n",
    "    0 & \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "> - In inner product $\\prod \\nolimits_{m=1}^{M} \\theta_{cdm}^{\\mathbb{I}(m=x_d^{\\text{new}})}$, only $\\theta_{cdx_d^{\\text{new}}}$ is multiplied and all the other multipliers are 1 due to the power of indicator function.\n",
    "    \n",
    "> - One thing to note is that the above classification criterion is the product of a series numbers smaller than 1 which will generate a rather small number. A better way is to take **logarithm** to transform product into summation and then compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Parameter Estimation\n",
    "\n",
    "- **Goal:** Given training data $\\mathcal{D} = \\{ (\\vec{x}_1, y_1), \\dots, (\\vec{x}_N, y_N) \\}$, estimate **class-conditional probabilities** $\\theta$ and **class priors** $\\pi$.\n",
    "\n",
    "\n",
    "- We will discuss the **MLE** and **MAP** parameter estimates.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "- The **likelihood** for a single data case $(\\vec{x}_n, y_n=c)$ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    & P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    &= P(y_n) \\prod \\nolimits_{d=1}^D P(x_{nd}|y_n) \\\\\n",
    "    &= \\prod \\nolimits_{c=1}^C P(y_n=c)^{\\I(y_n=c)} \\cdot \\prod \\nolimits_{c=1}^C \\prod \\nolimits_{d=1}^D \\prod \\nolimits_{m=1}^M P(x_{nd}=m|y_n=c)^{\\I(x_{nd}=m) \\I(y_n=c)}\\\\\n",
    "    &= \\prod \\nolimits_{c=1}^C \\pi_c^{\\I(y_n=c)} \\cdot \\prod \\nolimits_{c=1}^C \\prod \\nolimits_{d=1}^D \\prod \\nolimits_{m=1}^M \\theta_{cdm}^{\\I(x_{nd}=m) \\I(y_n=c)}\\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "- Therefore, the **log-likelihood** is\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    & \\log P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    & = \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c + \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{d=1}^D \\sum \\nolimits_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c) \\log \\theta_{cdm}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    \n",
    "- The **log-likelihood** for all training data $\\mathcal{D} = \\{ (\\vec{x}_n, y_n) \\}_{n=1}^N $ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    & \\log P(\\mathcal{D}| \\pi, \\theta)\\\\\n",
    "    &= \\log \\prod \\nolimits_{n=1}^N P((\\vec{x}_n, y_n) | \\pi, \\theta) = \\sum \\nolimits_{n=1}^N \\log P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    &= \\boxed{\\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c + \\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{d=1}^D \\sum \\nolimits_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c) \\log \\theta_{cdm}}\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "- With the constraints $\\sum_{c=1}^C \\pi_c=1$ and $\\sum_{m=1}^M \\theta_{cdm}=1$, we could maximize log-likelihood function $\\log P(\\mathcal{D}| \\pi, \\theta)$ using *Lagrange multiplier*. (Derivation is in the notes!)\n",
    "\n",
    "- By maximizing log-likelihood function, we could have maximum likelihood estimators:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cdm} = \\frac{N_{cdm}}{N_c}\n",
    "    $$\n",
    "    and\n",
    "    $$\n",
    "    \\hat{\\pi} = (\\hat{\\pi}_1, \\dots,\\hat{\\pi}_c, \\dots,\\hat{\\pi}_C); \\hat{\\theta}_{cd} = (\\hat{\\theta}_{cd1}, \\dots,\\hat{\\theta}_{cdm}, \\dots,\\hat{\\theta}_{cdM})\n",
    "    $$\n",
    "    - $N = $ Number of examples in $\\mathcal{D}$\n",
    "    - $N_c = $ Number of examples in class $c$ in $\\mathcal{D}$\n",
    "    - $N_{cdm} = $ Number of examples in class $c$ with $x_d = m$ in $\\mathcal{D}$\n",
    "    \n",
    "- Intuitive Interpretation\n",
    "    - The class prior $\\pi$ is obtained from the density of each class $\\{1, \\dots, C\\}$ in $\\mathcal{D}$\n",
    "    - The class-conditional probability $\\theta_{cd}$ is obtained from the density of $x_d \\in \\{1,\\dots,M \\}$ among all examples in class $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too complicated? Lets do Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.aylien.com/naive-bayes-for-dummies-a-simple-explanation/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
