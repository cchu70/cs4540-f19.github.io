{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 17:  Naive Bayes Algorithm\n",
    "\n",
    "* Instructor:  **Jacob Abernethy**\n",
    "* Date:  Thursday, October 25, 2018\n",
    "\n",
    "  TAs:  Naveen Kodali, Benjamin Bray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "- Basics\n",
    "    - Bayes Rule\n",
    "    - ML for binomial, multinomial distributions (http://www.math.utah.edu/~levin/M5080/mle.pdf)\n",
    "- Naive Bayes Classifier\n",
    "    - Independence Assumption\n",
    "    - MLE Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Rule \n",
    "\n",
    "P(A|B) = P(B|A)P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.a\n",
    "\n",
    "You go to see the doctor about an ingrowing toenail. The doctor selects you at random to have\n",
    "a blood test for swine flu, which for the purposes of this exercise we will say is currently suspected\n",
    "to affect 1 in 10,000 people in Australia. The test is 99% accurate, in the sense that the probability\n",
    "of a false positive is 1%. The probability of a false negative is zero. You test positive. What is the\n",
    "new probability that you have swine flu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.b\n",
    "\n",
    "Now imagine that you went to a friendâ€™s wedding in Mexico recently, and (for the purposes of this\n",
    "exercise) it is know that 1 in 200 people who visited Mexico recently come back with swine flu.\n",
    "Given the same test result as above, what should your revised estimate be for the probability you\n",
    "have the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML for binomial distribution (Exercise 2)\n",
    "\n",
    "Consider a series of N experiments with outcomes {y1,...yN}, where each y is in {0,1}. If we know that the probability of this sequence is given by a binomial distrbution with some unknown parameter p, what is the maximum likelihood estimate for p?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML for multinomial distribution (Exercise 3)\n",
    "\n",
    "Consider a series of N experiments with outcomes {y1,...yN}, where each y is in {1,2..,K}. If we know that the probability of this sequence is given by a multinomial distrbution with some unknown parameters {p1,..,pK}, what are the maximum likelihood estimates for each of these p_i?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Classification\n",
    "\n",
    "- Given some data X = {x1,...,xn} and their labels Y = {y1,...,yn} \\in {1,...,K}. The goal of classification is to find a function f: X -> Pr(Y=k|X) that fits this data and also performs well on unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Problem\n",
    "\n",
    "- We will use **Naive Bayes** to solve the following classification problem:\n",
    "    - **Categorical** feature vector $\\vx = (x_1, x_2, \\dots, x_D)$ with length $D$\n",
    "        - Each feature $x_d \\in \\{1, \\dots ,M \\}$, $\\forall d = 1, \\dots, D$\n",
    "    - Predict discrete class label $y \\in \\{1, 2, \\dots, C \\}$\n",
    "\n",
    "- For example, in **Spam Mail Classification**,\n",
    "    - Predict whether an email is `SPAM` ($y=1$) or `HAM` ($y=0$)\n",
    "    - Use words / metadata in the email as features\n",
    "    - For simplicity, we can use **bag-of-words** features,\n",
    "        - Assume fixed vocabulary $V$ of size $|V| = D$\n",
    "        - Feature $x_d$, for $d \\in \\{1, 2, \\dots, D \\}$, indicates the existence of $d\\text{th}$ word in the email\n",
    "        - Eg. $x_d = 1$ if $d\\text{th}$ word is in the email; $x_d = 0$ otherwise\n",
    "        - In this case $M=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Independence Assumption and Full model\n",
    "\n",
    "- The essence of Naive Bayes is the **conditionally independence assumption**\n",
    "    $$\n",
    "    P(\\vx | y = c) = \\prod_{d=1}^D P(x_d | y=c)\n",
    "    $$\n",
    "    i.e., given the label, all features are independent.\n",
    "    \n",
    "- The **full generative** model of Naive Bayes is:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y       &\\sim \\mathrm{Categorical}(\\pi) \\\\\n",
    "    x_d | y=c &\\sim \\mathrm{Categorical}(\\theta_{cd}) \\quad \\forall\\, d = 1,\\dots,D\n",
    "    \\end{align}\n",
    "    $$\n",
    "    with parameters:\n",
    "    - **Class priors** $\\pi = (\\pi_1, \\dots, \\pi_C) \\in \\Delta^C$, \n",
    "        - i.e. $P(y = c) = \\pi_c$, $\\forall c = 1,\\dots,C $\n",
    "        - $\\Delta^C$ is C-**simplex**. $\\pi \\in \\Delta^C$ is saying that $\\sum_{c=1}^C \\pi_c = 1$ and $\\pi_c \\geq 0, \\forall c=1,\\dots,C$\n",
    "    - **Class-conditional probabilities** $\\theta_{cd} = (\\theta_{cd1},  \\dots, \\theta_{cdM}) \\in \\Delta^M$\n",
    "        - i.e. $P(x_d = m| y = c) = \\theta_{cdm}$ for every $d = 1,\\dots,D, m = 1, \\dots, M, c = 1, \\dots, C$\n",
    "\n",
    "- Parameter $\\pi$ and $\\theta$ are learned from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ">**Remark**\n",
    "> - **NOTE** in definition and derivation of this lecture, we assume a more general case $x_d \\in \\{1, \\dots ,M \\}$ of which $M>2$. But in spam email classification and the derivation in textbook, binary feature, i.e. $M=2$, is used. So don't get confused!\n",
    "\n",
    "> - When $M=2$, $x_d | y=c$ is also Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Prediction\n",
    "\n",
    "- Given the independence assumption and full model, for some new data $\\vx^{\\text{new}} = (x_1^{\\text{new}}, \\dots, x_D^{\\text{new}})$ we will classify based on\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(y=c|\\vx = \\vx^{\\text{new}}) \\\\\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(\\vx = \\vx^{\\text{new}} | y=c) P(y=c) \\\\\n",
    "    &=\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} P(y=c) \\prod \\nolimits_{d=1}^{D} P(x_d = x_d^{\\text{new}} | y=c) \\\\\n",
    "    &=\\boxed{\\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\theta_{cdx_d^{\\text{new}}}} \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "- If we assume $x_d^{\\text{new}} \\in \\{1,\\dots,M \\}, \\forall d = 1,\\dots,D$, we could also express the above expression equivalently using **indicator function**\n",
    "    $$\n",
    "    y = \\underset{c \\in \\{1,\\dots,C\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\prod \\nolimits_{m=1}^{M} \\theta_{cdm}^{\\mathbb{I}(m=x_d^{\\text{new}})}\n",
    "    $$\n",
    "    \n",
    "- So as long as we learned parameter $\\pi$ and $\\theta$, we could classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> **Remark**\n",
    "\n",
    "> - Indicator function\n",
    "    $$\n",
    "    \\mathbb{I}(m=x_d^{\\text{new}}) = \n",
    "    \\begin{cases}\n",
    "    1 & \\text{ if } m=x_d^{\\text{new}}\\\\ \n",
    "    0 & \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "> - In inner product $\\prod \\nolimits_{m=1}^{M} \\theta_{cdm}^{\\mathbb{I}(m=x_d^{\\text{new}})}$, only $\\theta_{cdx_d^{\\text{new}}}$ is multiplied and all the other multipliers are 1 due to the power of indicator function.\n",
    "    \n",
    "> - One thing to note is that the above classification criterion is the product of a series numbers smaller than 1 which will generate a rather small number. A better way is to take **logarithm** to transform product into summation and then compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Parameter Estimation\n",
    "\n",
    "- **Goal:** Given training data $\\mathcal{D} = \\{ (\\vec{x}_1, y_1), \\dots, (\\vec{x}_N, y_N) \\}$, estimate **class-conditional probabilities** $\\theta$ and **class priors** $\\pi$.\n",
    "\n",
    "\n",
    "- We will discuss the **MLE** and **MAP** parameter estimates.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "- The **likelihood** for a single data case $(\\vec{x}_n, y_n=c)$ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    & P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    &= P(y_n) \\prod \\nolimits_{d=1}^D P(x_{nd}|y_n) \\\\\n",
    "    &= \\prod \\nolimits_{c=1}^C P(y_n=c)^{\\I(y_n=c)} \\cdot \\prod \\nolimits_{c=1}^C \\prod \\nolimits_{d=1}^D \\prod \\nolimits_{m=1}^M P(x_{nd}=m|y_n=c)^{\\I(x_{nd}=m) \\I(y_n=c)}\\\\\n",
    "    &= \\prod \\nolimits_{c=1}^C \\pi_c^{\\I(y_n=c)} \\cdot \\prod \\nolimits_{c=1}^C \\prod \\nolimits_{d=1}^D \\prod \\nolimits_{m=1}^M \\theta_{cdm}^{\\I(x_{nd}=m) \\I(y_n=c)}\\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "- Therefore, the **log-likelihood** is\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    & \\log P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    & = \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c + \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{d=1}^D \\sum \\nolimits_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c) \\log \\theta_{cdm}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    \n",
    "- The **log-likelihood** for all training data $\\mathcal{D} = \\{ (\\vec{x}_n, y_n) \\}_{n=1}^N $ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    & \\log P(\\mathcal{D}| \\pi, \\theta)\\\\\n",
    "    &= \\log \\prod \\nolimits_{n=1}^N P((\\vec{x}_n, y_n) | \\pi, \\theta) = \\sum \\nolimits_{n=1}^N \\log P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    &= \\boxed{\\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c + \\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{d=1}^D \\sum \\nolimits_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c) \\log \\theta_{cdm}}\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "- With the constraints $\\sum_{c=1}^C \\pi_c=1$ and $\\sum_{m=1}^M \\theta_{cdm}=1$, we could maximize log-likelihood function $\\log P(\\mathcal{D}| \\pi, \\theta)$ using *Lagrange multiplier*. (Derivation is in the notes!)\n",
    "\n",
    "- By maximizing log-likelihood function, we could have maximum likelihood estimators:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cdm} = \\frac{N_{cdm}}{N_c}\n",
    "    $$\n",
    "    and\n",
    "    $$\n",
    "    \\hat{\\pi} = (\\hat{\\pi}_1, \\dots,\\hat{\\pi}_c, \\dots,\\hat{\\pi}_C); \\hat{\\theta}_{cd} = (\\hat{\\theta}_{cd1}, \\dots,\\hat{\\theta}_{cdm}, \\dots,\\hat{\\theta}_{cdM})\n",
    "    $$\n",
    "    - $N = $ Number of examples in $\\mathcal{D}$\n",
    "    - $N_c = $ Number of examples in class $c$ in $\\mathcal{D}$\n",
    "    - $N_{cdm} = $ Number of examples in class $c$ with $x_d = m$ in $\\mathcal{D}$\n",
    "    \n",
    "- Intuitive Interpretation\n",
    "    - The class prior $\\pi$ is obtained from the density of each class $\\{1, \\dots, C\\}$ in $\\mathcal{D}$\n",
    "    - The class-conditional probability $\\theta_{cd}$ is obtained from the density of $x_d \\in \\{1,\\dots,M \\}$ among all examples in class $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too complicated? Lets do Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://blog.aylien.com/naive-bayes-for-dummies-a-simple-explanation/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
