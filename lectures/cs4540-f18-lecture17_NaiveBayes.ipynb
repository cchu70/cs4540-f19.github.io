{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\vx}{\\vec{x}}\n",
    "\\newcommand{\\I}{\\mathbb{I}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# Lecture 17:  Bayes Rule, Binomial, Regression, MAP Estimation\n",
    "\n",
    "Naveen Kodali and Jacob Abernethy\n",
    "*Date:  Thursday, October 25, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "- Basics\n",
    "    - Bayes Rule\n",
    "    - MLE for binomial, multinomial distributions\n",
    "    - MLE for linear regression\n",
    "    - MAP estimation for linear regression\n",
    "- Naive Bayes Classifier\n",
    "    - Independence Assumption\n",
    "    - MLE Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Rule \n",
    "\n",
    "P(A|B) = P(B|A)P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.a\n",
    "\n",
    "You go to see the doctor about an ingrowing toenail. The doctor selects you at random to have\n",
    "a blood test for swine flu, which for the purposes of this exercise we will say is currently suspected\n",
    "to affect 1 in 10,000 people in Australia. The test is 99% accurate, in the sense that the probability\n",
    "of a false positive is 1%. The probability of a false negative is zero. You test positive. What is the\n",
    "new probability that you have swine flu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1.b\n",
    "\n",
    "Now imagine that you went to a friendâ€™s wedding in Mexico recently, and (for the purposes of this\n",
    "exercise) it is know that 1 in 200 people who visited Mexico recently come back with swine flu.\n",
    "Given the same test result as above, what should your revised estimate be for the probability you\n",
    "have the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML for binomial distribution (Exercise 2)\n",
    "\n",
    "\n",
    "\n",
    "Consider a series of N experiments with outcomes $\\{y_1,...y_N\\}$, where each $y_i$ is in $\\{0,1\\}$. The probability of this sequence is given by a binomial distrbution with some unknown parameter $p$ (the probability that $y_i = 1$). That is, if $k = y_1 + \\ldots + y_N$, then\n",
    "$$P(k | p) := \\binom{N}{k} p^k (1-p)^{N-k}$$\n",
    "\n",
    "Given the data, what is the maximum likelihood estimate for $p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML for multinomial distribution (Exercise 3)\n",
    "\n",
    "Consider a series of N experiments with outcomes$\\{y_1,...y_N\\}$, where each $y_i$ is in $\\{1,2..,K\\}$. We know that the probability of this sequence is given by a multinomial distrbution with some unknown parameters $(p_1,..,p_K)$, i.e. the probability of $y_i = j$ is given by $p_j$. If we write $x_j$ to be the *count* of the number of times $y_i = j$, then probability distribution is given by\n",
    "$$P(x_1, \\ldots, x_K | p_1, \\ldots, p_K) := \\binom{n}{x_1, \\ldots, x_K} p_1^{x_1} \\cdots p_K^{x_K}$$\n",
    "\n",
    "\n",
    "Given the data, what are the maximum likelihood estimates for each of these p_i?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood: Simple Linear Regression\n",
    "\n",
    "One of the most basic techniques for trying to estimate a real number $y$ given an observed vector $x \\in \\R^d$ is known as *linear regression*, aka *least squares*. The key idea is to assume that the data $x_1, \\ldots, x_n$ are fixed vectors, and the labels $y_1, \\ldots, y_n$ are *independent* random real numbers whose probabiliy distribution is *guassian*. For some parameter $\\theta \\in \\R^d$ we have\n",
    "$$P( y_i | x_i, \\theta) = \\mathcal{N}(y_i | x_i^\\top \\theta, \\sigma^2)= \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^\\top \\theta)^2}{2\\sigma^2}\\right)$$\n",
    "What is the MLE for $\\theta$? (You don't necessarily have to find a closed-form solution)\n",
    "\n",
    "Useful fact: by independence we have $P(y_1, \\ldots, y_n | x_1, \\ldots, x_n, \\theta) = P(y_1 | x_1, \\theta) \\cdots P(y_n | x_n, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum A Posterior estimation\n",
    "\n",
    "Previously, we have been looking at the Max. Likelihood estimator. A more advanced estimator involves a *prior* on the parameters. This means that we \"prefer\", a priori, certain parameters over others. In other words, we think some parameters are more likely to be true than others. You describe your \"preference\" for some parameters using a prior distribution $P(\\theta)$. Given data $x_1, \\ldots, x_n$, the Maximum A Posterior (MAP) estimate for $\\theta$ is then\n",
    "$$\\arg\\max_\\theta P(\\theta) \\prod_{i=1}^n P(x_i | \\theta) \\quad \\text{ equiv. to } \\quad \n",
    "\\arg\\max_\\theta \\log P(\\theta) +  \\sum_{i=1}^n \\log P(x_i | \\theta) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "For the linear regression model above, we can add a prior to the parameter vector $\\theta$. The standard prior is the so-called \"multivariate gaussian\" with a single *fixed* parameter $\\nu$, i.e.\n",
    "$$P(\\theta) = \\frac{1}{(2\\pi \\nu^2)^{d/2}} \\exp\\left(\\frac{\\|\\theta\\|^2}{2\\nu^2}\\right)$$\n",
    "\n",
    "The likelihood function is still the same:\n",
    "$P( y_i | x_i, \\theta) =\n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^\\top \\theta)^2}{2\\sigma^2}\\right)$. \n",
    "\n",
    "**Problem**: What is the MAP estimate for this model?\n",
    "$$\\arg\\max_\\theta \\log P(\\theta) +  \\sum_{i=1}^n \\log P(y_i | x_i, \\theta)$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
