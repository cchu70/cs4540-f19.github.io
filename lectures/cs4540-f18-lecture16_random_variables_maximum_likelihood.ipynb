{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "c11f7b86-dba3-4e8d-afbf-bd147373e999"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np;\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9707fbb8-a773-42d6-982b-ee1f3c961f53"
    }
   },
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\E}{\\mathbb{E}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\grad}{\\nabla}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d47522cd-d9c5-4612-87a1-e2c1dfdb40d6"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# L16: Computing Eigenvalues & Eigenvectors\n",
    "\n",
    "*Tuesday, October 16, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "* Some review of probability and statistics\n",
    "    * Random variables\n",
    "    * Expectation\n",
    "    * Variance\n",
    "* Maximum Likelihood Estimation\n",
    "    * The basics\n",
    "    * Some examples\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a random variable? Discreet case.\n",
    "\n",
    "Technically speaking, a random variable $X : \\Omega \\to \\R$ is just a map from some *sample space* $\\Omega$ to the real numbers. We assume we have a *probability measure* $\\Omega$.\n",
    "\n",
    "When $\\Omega$ is a finite or countably-infinite set, then we can assume our measure is just a probability $p(\\omega)$ assigned to every $\\omega \\in \\Omega$, where $\\sum_{\\omega \\in \\Omega} p_\\omega = 1$.\n",
    "* We often will write $P(X = a)$ which is precisely $\\sum_{\\omega \\in \\Omega : X(\\omega) = 1} p(\\omega)$.\n",
    "* The expectation of a random variable $\\E[X]$ is defined as $\\sum_{\\omega \\in \\Omega} X(\\omega) p(\\omega)$.\n",
    "* Similarly, the expectation of a function $\\E[g(X)] = \\sum_{\\omega \\in \\Omega} g(X(\\omega)) p(\\omega)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a random variable? Continuous case.\n",
    "\n",
    "When the sample space $\\Omega$ is uncountably infinite, we usually need to go to full measure theory to talk about random variables in general. We won't do today, but consider a measure theory course!\n",
    "\n",
    "Countinuous random variables are the most common in Machine Learning. The best way to think about random variables is through their *probability density function* and *cumulative density function*\n",
    "* For random variable $X$, the CDF is $F(x) := P(X \\leq x)$\n",
    "* Also, the PDF of $X$ is the derivative of the CDF, $f(x) := F'(x)$\n",
    "* WARNING: not every random variable has a PDF! But it always has a CDF! (But CDF may not be diff'bl)\n",
    "* When a r.v. has a PDF $f(\\cdot)$, we can write\n",
    "$$ \\E[X] = \\int x f(x)\\, dx $$\n",
    "* When $\\mu$ is the mean of random variable $X$, then the variance $\\text{Var}(X)$ is $\\E[(X-\\mu)^2]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PDF vs CDF\n",
    "\n",
    "<img src=\"images/pdf_cdf.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Independence\n",
    "\n",
    "* Strictly speaking, two random variables $X$ and $Y$ are independent if for any (measureable) sets $A, B \\subset \\R$ we have $P(X \\in A \\text{ and } Y \\in B) = P(X \\in A) P( Y \\in B)$.\n",
    "* Perhaps the most useful fact of independence of $X$ and $Y$ is that $\\E[XY] = \\E[X]\\E[Y]$\n",
    "<div style=\"padding:10px; margin:10px; border: 1px solid black\">\n",
    "<b>Problem:</b> Show the following<br>\n",
    "<b>Part A:</b> Let $X$ be any random variable. Show that $\\text{Var}(X) = (\\E[X])^2 - \\E[X^2]$<br>\n",
    "<b>Part B:</b> Let $X$ be some random variable. Then $\\text{Var}(X + X + X) = 9 \\text{Var}(X)$<br>\n",
    "<b>Part C:</b> Let $X_1, \\ldots, X_n$ be *independent* random variables. Then $\\text{Var}(X_1 + \\ldots + X_n) = \\text{Var}(X_1) + \\ldots + \\text{Var}(X_n)$<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Markov's Inequality\n",
    "\n",
    "Try to show the famous Markov's Inequality:\n",
    "\n",
    "Let $X$ be a random variable that only takes positive values. Then for any $k > 0$ we have\n",
    "$$P(X \\geq k) \\leq \\frac{\\E[X]}{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Chebyshev's Inequality\n",
    "\n",
    "Now try to prove the same for Chebyshev:\n",
    "\n",
    "Let $X$ be any random variable with mean $\\E[X] = \\mu$, and a bounded variance. Then for any $k > 0$ we have\n",
    "$$P(|X - \\mu| \\geq k) \\leq \\frac{\\text{Var}(X)}{k^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Random Variable Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameterized families of probability distributions\n",
    "\n",
    "* Generally, we assume that the data we observe in the real world are drawn from some unknown distribution\n",
    "* Much of the research in statistics and ML can be viewed as trying to *reason about uncertainty* by estimating these distributions, or properties of these distributions\n",
    "* For example: what is the *mean*? What is the *median*? What is the threshold of the upper decile? Given that you are in the top 30% of the distribution, what is your most likely value? Etc.\n",
    "* Most distributions we work with come from *parameterized families*, i.e. we can describe the distribution as a function of some inputs, known as parameters\n",
    "* Most basic example:\n",
    "    * the *gaussian* distribution has PDF \n",
    "    $$p_{\\mu, \\sigma^2}(x) := \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "    * this is a *function of $x$* but it is *parameterized by* $\\mu$ and $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributions of interest:\n",
    "\n",
    "\n",
    "**Discrete distributions**\n",
    "* Bernoulli: https://en.wikipedia.org/wiki/Bernoulli_distribution\n",
    "* Binomial: https://en.wikipedia.org/wiki/Binomial_distribution\n",
    "* Poisson: https://en.wikipedia.org/wiki/Poisson_distribution\n",
    "\n",
    "**Continuous distributions**\n",
    "* Gaussian/Normal: https://en.wikipedia.org/wiki/Normal_distribution\n",
    "* Uniform: https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)\n",
    "* Exponential: https://en.wikipedia.org/wiki/Exponential_distribution\n",
    "* Beta: https://en.wikipedia.org/wiki/Beta_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating Parameters of Distributions: MLE\n",
    "\n",
    "Let us say we have $n$ independent draws $X_1, \\ldots, X_n$ from some distribution $p_\\theta$ parameterized by $\\theta$. How might we estimate $\\theta$?\n",
    "\n",
    "**Maximum Likelihood Estimation** (A natural heuristic): Choose the $\\theta$ that makes the data most likely! That is, it should maximize the density (PDF) of the samples\n",
    "\n",
    "(Warning: this is a very popular but *controversial* approach to parameter estimation. Just as a Bayesian what they think about the MLE! We won't get into this debate here)\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{MLE}} = \\arg\\max_\\theta p_\\theta(X_1, \\ldots, X_n) = \\arg\\max_\\theta \\prod_{i=1}^n p_\\theta(X_i)\n",
    "$$\n",
    "It's often easier to take the $\\log$ of the product, since it doesn't change the argmax!\n",
    "$$\\theta_{\\text{MLE}} = \\arg\\max_\\theta \\sum_{i=1}^n \\log p_\\theta(X_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: MLE of the exponential distribution\n",
    "\n",
    "Assume $X_1, \\ldots, X_n > 0$ are sampled from an exponential distribution $p_\\lambda(x) = \\lambda \\exp(\\lambda x)$ with. What is the MLE for the $\\lambda$?\n",
    "\n",
    "Also, what is the mean of the distribution $p_\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: MLE of the Bernoulli distribution\n",
    "\n",
    "Assume $X_1, \\ldots, X_n$ are sampled from a Bernoulli distribution, which is supported on $\\{0,1\\}$: $p_\\theta(x) =  \\theta^x (1-\\theta)^{1-x}$ with. What is the MLE for $\\theta$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: MLE of the Poisson distribution\n",
    "\n",
    "Assume $X_1, \\ldots, X_n > 0$ are nonnegative integers sampled from the Poisson distribution: $p_\\lambda(k) = \\frac{\\lambda^k\\exp(-\\lambda)}{k!}$ with. Note that this is a *discrete* distribution. What is the MLE for the $\\lambda$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: MLE of the Gaussian Distribution\n",
    "\n",
    "Assume $X_1, \\ldots, X_n$ are real numbers sampled from a Gaussian distribution:\n",
    "$$p_{\\mu, \\sigma^2}(x) := \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "What are the MLE estimates of $\\mu$ and $\\sigma^2$? *Hint*: solve for $\\mu$ first."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
