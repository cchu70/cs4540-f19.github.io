{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\vx}{\\vec{x}}\n",
    "\\newcommand{\\I}{\\mathbb{I}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# Lecture 19: Logistic Regression\n",
    "\n",
    "Naveen Kodali and Jacob Abernethy\n",
    "*Date:  Tuesday, November 6, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Problem\n",
    "\n",
    "- We will use **Naive Bayes** to solve the following classification problem:\n",
    "    - **Categorical** feature vector $\\vx = (x_1, x_2, \\dots, x_D)$ with length $D$\n",
    "        - Each feature $x_d \\in \\{0,1\\}$, $\\forall d = 1, \\dots, D$\n",
    "        - Note: you can allow for non-binary features - $x_d \\in \\{0,1, \\ldots M\\}$\n",
    "    - Predict discrete class label $y \\in \\{1, 2, \\dots, C \\}$\n",
    "\n",
    "- For example, in **Spam Mail Classification**,\n",
    "    - Predict whether an email is `SPAM` ($y=1$) or `HAM` ($y=0$)\n",
    "    - Use words / metadata in the email as features\n",
    "    - For simplicity, we can use **bag-of-words** features,\n",
    "        - Assume fixed vocabulary $V$ of size $|V| = D$\n",
    "        - Feature $x_d$, for $d \\in \\{1, 2, \\dots, D \\}$, indicates the existence of $d\\text{th}$ word in the email\n",
    "        - Eg. $x_d = 1$ if $d\\text{th}$ word is in the email; $x_d = 0$ otherwise\n",
    "        - In this case $M=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Independence Assumption and Full model\n",
    "\n",
    "- The essence of Naive Bayes is the **conditionally independence assumption**\n",
    "    $$\n",
    "    P(\\vx | y = c) = \\prod_{d=1}^D P(x_d | y=c)\n",
    "    $$\n",
    "    i.e., given the label, all features are independent.\n",
    "    \n",
    "- The **full generative** model of Naive Bayes is:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    P(y = c ) & = \\pi_c \\quad \\forall\\, c=0,1 \\\\\n",
    "    P(x_d = 1 | y = c ) &= \\theta_{cd} \\quad \\forall\\, d = 1,\\dots,D\n",
    "    \\end{align}\n",
    "    $$\n",
    "- Parameter $\\pi$ and $\\theta$ are learned from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Prediction\n",
    "\n",
    "- Given the independence assumption and full model, for some new data $\\vx^{\\text{new}} = (x_1^{\\text{new}}, \\dots, x_D^{\\text{new}})$ we will classify based on\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(y=c|\\vx = \\vx^{\\text{new}}) \\\\\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(\\vx = \\vx^{\\text{new}} | y=c) P(y=c) \\\\\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(y=c) \\prod \\nolimits_{d=1}^{D} P(x_d = x_d^{\\text{new}} | y=c) \\\\\n",
    "    &=\\boxed{\\underset{c \\in \\{0,1\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\theta_{cd}^{x_d^{\\text{new}}} (1-\\theta_{cd})^{1-x_d^{\\text{new}}}} \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "- So as long as we learned parameter $\\pi$ and $\\theta$, we could classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "\n",
    "- We have alread solved the MLE for the multinomial distribution (categorical variable)! We observed that:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cd} = \\frac{N_{cd}}{N_c}\n",
    "    $$\n",
    "    where\n",
    "    - $N = $ Number of examples in $\\mathcal{D}$\n",
    "    - $N_c = $ Number of examples in class $c$ in $\\mathcal{D}$\n",
    "    - $N_{cd} = $ Number of examples in class $c$ with $x_d = 1$\n",
    "    \n",
    "- Intuitive Interpretation\n",
    "    - The class prior $\\pi$ is obtained from the density of each class $\\{1, \\dots, C\\}$ in $\\mathcal{D}$\n",
    "    - The class-conditional probability $\\theta_{cd}$ is obtained from the density of $x_d \\in \\{0,1\\}$ among all examples in class $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example dataset\n",
    "#### Example No.  |  Color  |  Type  |  Origin  |  Stolen?\n",
    "\n",
    "1 | Red | Sports | Domestic | Yes <br>\n",
    "2 | Red | Sports | Domestic | No <br>\n",
    "3 | Red | Sports | Domestic | Yes <br>\n",
    "4 | Yellow | Sports | Domestic | No <br>\n",
    "5 | Yellow | Sports | Imported | Yes <br>\n",
    "6 | Yellow | SUV | Imported | No <br>\n",
    "7 | Yellow | SUV | Imported | Yes <br>\n",
    "8 | Yellow | SUV | Domestic | No <br>\n",
    "9 | Red | SUV | Imported | No <br>\n",
    "10 | Red | Sports | Imported | Yes \n",
    "\n",
    "### Problem\n",
    "\n",
    "What is MLE of the parameters $\\theta$ and $\\mu$? \n",
    "\n",
    "For these parameters, what is P(Yes | Red Domestic SUV) and (No | Red Domestic SUV)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MAP Estimation for Naive Bayes with Beta Prior\n",
    "\n",
    "In the above example, what if we never see a red car that is stolen (perhaps because we didn't have much data)? What will be $P(\\text{Stolen} | \\text{Red Imported Sports})$? The predicted probability will be 0! This is not desireable, since it would essentially be \"overfitting\" to the data.\n",
    "\n",
    "This is where we want a prior distribution. As we discussed previously, it's best to use a conjugate prior if you can, because the calculations are very convenient. The conjugate distribution to the binomial model is the *beta distribution*, parameterized by $\\alpha, \\beta > 0$:\n",
    "$$P(\\theta | \\alpha, \\beta) := \\frac{\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}$$\n",
    "where the normalization term $B$ is defined in terms of the [gamma function](https://en.wikipedia.org/wiki/Gamma_function), $B(\\alpha, \\beta) := \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum a Posteriori\n",
    "\n",
    "\n",
    "- We have alread solved the MLE for Naive Bayes:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cd} = \\frac{N_{cd}}{N_c}\n",
    "    $$\n",
    "    where $N = $ #examples in the dataset, $N_c = $ #examples in class $c$ in dataset, $N_{cd} = $ #examples in class $c$ with $x_d = 1$\n",
    "    \n",
    "*Problem*: What is the MAP estimate of the parameters $\\theta_{cd}$ for this model, when we assume the prior on every $\\theta_{cd}$ is (independently) distributed according to $\\text{Beta}(\\alpha,\\beta)$?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Answer\n",
    "\n",
    "You get the \"smoothed\" version of the counts:\n",
    "    $$\n",
    "     \\hat{\\theta}_{cd}^{\\text{MAP}(\\alpha,\\beta)} = \\frac{N_{cd} + \\alpha}{N_c + \\alpha + \\beta}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary is to Categorical as Beta is to Dirichlet\n",
    "\n",
    "We want to recall that the binomial model is a distribution on (counts of) *binary* variables. In the example above, all of the features $x_d$ took one of two values, and the class label YES/NO was also binary. In this case, you only need $\\theta_{cd}$ for each class $c \\in \\{0,1\\}$ and feature index $d = 1, \\ldots, D$\n",
    "\n",
    "*BUT* what if the features and classes can take one of more than two classes? In this case, we would assume our features and classes are *categorical* variables, and we would use the *multinomial* distribution to model them. If $X$ is a categorical variable with parameter vector $q$, then $P(X = j|q) = q_j$.\n",
    "\n",
    "The Beta prior is the \"good\" prior for the binomial distribution. For the categorical, it's the *Dirichlet* distribution. Let $q \\in \\Delta_K$ be some probability distribution on $K$ classes. Given parameters $\\alpha_1, \\ldots, \\alpha_K$, the Dirichlet distribution for $\\vec \\alpha$ has PDF:\n",
    "$$P_{\\vec \\alpha}(q) := \\frac{1}{B(\\vec \\alpha)} \\prod_{i=1}^K q_i^{\\alpha_i - 1}$$\n",
    "where $B(\\vec \\alpha) := \\frac{\\prod_{i=1}^K \\Gamma(\\alpha_i)}{\\Gamma(\\sum_{i=1}^K \\alpha_i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Part 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our models so far:\n",
    "\n",
    "+ Linear regression\n",
    "    + Tries to find a model to predict $y$ from $x \\in \\R^d$, with parameters $\\theta \\in \\R^d$\n",
    "    + The variable $y$ is a *real* number\n",
    "    + Model assumes that $y$ is Gaussian with mean $x^\\top \\theta$ and variance $\\sigma^2$\n",
    "    + Model does **not** treat $x$ as a random variable\n",
    "+ Naive Bayes\n",
    "    + Tries to find a model to predict $y$ from $x$, with parameters $\\theta, \\mu$\n",
    "    + (Typically) the variable $y$ is a *categorical* variable\n",
    "    + (Typically) the coordinates of $x$ are *categorical* variables\n",
    "    + Model **does** treat $x$ as a random variable\n",
    "    + The model makes the (somewhat unusual) assumption that features $x = (x_1, \\ldots, x_D)$ are *independent once we condition on $y$*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## New Model: Logistic Regression\n",
    "\n",
    "+ Logistic regression\n",
    "    + Tries to find a model to predict $y$ from $x \\in \\R^d$, with parameters $\\theta \\in \\R^d$\n",
    "    + Model assumes that variable $y$ is a *categorical* variable with parameters $q$\n",
    "    + The prediction for $y$ is a probability distribution $q = P(y=1|x,\\theta), \\ldots, P(y=K|x,\\theta)$\n",
    "    + Model does **not** treat $x$ as a random variable\n",
    "    + Despite the name, LR is really a *classification* algorithm, not a regression alg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The likelihood model for Logistic Regression\n",
    "\n",
    "In this lecture, let us assume that the categorical label $y$ is *binary*, indeed $y \\in \\{-1,1\\}$, just to make things easier. But it is not hard to generalize this to multiclass (categorical > 2) settings.\n",
    "\n",
    "Given input $x$ and parameters $\\theta$, the prediction for $y$ can be viewed as a probability $q$ where\n",
    "$$q := P(y = 1 | x, \\theta) = \\frac{1}{1 + \\exp(-x^\\top \\theta)} = \\frac{\\exp(x^\\top \\theta)}{1 + \\exp(x^\\top \\theta)}.$$\n",
    "\n",
    "What's going on here? The problem is that $\\theta^\\top x$ is not guaranteed to be in $[0,1]$. We need to \"squash\" the real number $\\theta^\\top x$ into the probability space $[0,1]$. The standard way to do this is with the *sigmoid* function $\\sigma(s) = \\frac{1}{1 + \\exp(-s)}$.\n",
    "\n",
    "A nice property of the sigmoid: $\\sigma(-s) = (1-\\sigma(s))$. This is used a lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Log odds ratio\n",
    "\n",
    "It's often interesting to define what we call the *log-odds ratio* of a probability $p$ as $\\log \\frac{p}{1-p}$.\n",
    "\n",
    "\n",
    "With this in mind, we can also state this the Logistic Regression model as follows: the *log-odds ratio* is linear in $x$. That is, we have\n",
    "$$\\log \\frac{P(y = 1 | x, \\theta)}{P(y = -1 | x, \\theta)} = \\theta^\\top x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Challenge for LR: No closed form MLE!\n",
    "\n",
    "The maximum likelihood estimator for the logistic regression is the following:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\theta^{\\text{MLE}} & := & \\arg\\max_{\\theta \\in \\R^d} \\sum_{i=1}^n \\log P(y_i | x_i, \\theta) \\\\\n",
    "& = & \\arg\\max_{\\theta \\in \\R^d} \\sum_{i=1}^n \\log \\left(\\sigma(x_i^\\top \\theta)^{\\mathbb{I}(y_i = 1)}(1-\\sigma(x_i^\\top \\theta))^{\\mathbb{I}(y_i = -1)}\\right) \\\\\n",
    "& = & \\arg\\max_{\\theta \\in \\R^d} \\sum_{i=1}^n \\log \\sigma(y_i(x_i^\\top \\theta))\n",
    "\\end{eqnarray*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say h(x) = sigmoid(θ^T x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L(θ) = ∑_i ( −y_i log(h(x_i)) − (1−y_i) log(1−h(x_i)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD Step co-ord wise: θ_j = θ_j − α(h(x_i)−y_i)x_i^j\n",
    "Full Gradient matrix form: ▽_θ L(θ) = 1/m ∑_i (h(x_i)−y_i) x_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton Step: \n",
    "    H = 1/m ∑_i { h(x_i)(1−h(x_i)) x_i x_i^T }\n",
    "    θ = θ− H^(−1) ▽_θ L(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/cs4540-f18/cs4540-f18.github.io/master/lectures/logistic_x.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66562014]\n",
      "[0.66518165]\n",
      "[0.66473473]\n",
      "[0.66428829]\n",
      "[0.66384282]\n",
      "[0.66339833]\n",
      "[0.66295481]\n",
      "[0.66251226]\n",
      "[0.66207069]\n",
      "[0.66163008]\n",
      "[0.66119044]\n",
      "[0.66075177]\n",
      "[0.66031406]\n",
      "[0.6598773]\n",
      "[0.65944151]\n",
      "[0.65900667]\n",
      "[0.65857279]\n",
      "[0.65813985]\n",
      "[0.65770787]\n",
      "[0.65727683]\n",
      "[0.65684674]\n",
      "[0.6564176]\n",
      "[0.65598939]\n",
      "[0.65556212]\n",
      "[0.65513579]\n",
      "[0.6547104]\n",
      "[0.65428593]\n",
      "[0.6538624]\n",
      "[0.6534398]\n",
      "[0.65301812]\n",
      "[0.65259737]\n",
      "[0.65217754]\n",
      "[0.65175863]\n",
      "[0.65134064]\n",
      "[0.65092356]\n",
      "[0.6505074]\n",
      "[0.65009215]\n",
      "[0.64967781]\n",
      "[0.64926438]\n",
      "[0.64885186]\n",
      "[0.64844023]\n",
      "[0.64802951]\n",
      "[0.64761969]\n",
      "[0.64721077]\n",
      "[0.64680274]\n",
      "[0.64639561]\n",
      "[0.64598937]\n",
      "[0.64558402]\n",
      "[0.64517955]\n",
      "[0.64477597]\n",
      "[0.64437327]\n",
      "[0.64397146]\n",
      "[0.64357052]\n",
      "[0.64317046]\n",
      "[0.64277128]\n",
      "[0.64237297]\n",
      "[0.64197553]\n",
      "[0.64157896]\n",
      "[0.64118326]\n",
      "[0.64078842]\n",
      "[0.64039445]\n",
      "[0.64000134]\n",
      "[0.63960909]\n",
      "[0.63921769]\n",
      "[0.63882715]\n",
      "[0.63843746]\n",
      "[0.63804863]\n",
      "[0.63766064]\n",
      "[0.6372735]\n",
      "[0.63688721]\n",
      "[0.63650176]\n",
      "[0.63611715]\n",
      "[0.63573338]\n",
      "[0.63535044]\n",
      "[0.63496835]\n",
      "[0.63458708]\n",
      "[0.63420665]\n",
      "[0.63382705]\n",
      "[0.63344828]\n",
      "[0.63307033]\n",
      "[0.6326932]\n",
      "[0.6323169]\n",
      "[0.63194142]\n",
      "[0.63156676]\n",
      "[0.63119291]\n",
      "[0.63081987]\n",
      "[0.63044765]\n",
      "[0.63007624]\n",
      "[0.62970564]\n",
      "[0.62933584]\n",
      "[0.62896685]\n",
      "[0.62859866]\n",
      "[0.62823127]\n",
      "[0.62786468]\n",
      "[0.62749889]\n",
      "[0.62713389]\n",
      "[0.62676969]\n",
      "[0.62640628]\n",
      "[0.62604365]\n",
      "[0.62568182]\n",
      "[0.62532077]\n",
      "[0.6249605]\n",
      "[0.62460102]\n",
      "[0.62424232]\n",
      "[0.62388439]\n",
      "[0.62352724]\n",
      "[0.62317087]\n",
      "[0.62281527]\n",
      "[0.62246043]\n",
      "[0.62210637]\n",
      "[0.62175308]\n",
      "[0.62140055]\n",
      "[0.62104878]\n",
      "[0.62069778]\n",
      "[0.62034753]\n",
      "[0.61999805]\n",
      "[0.61964932]\n",
      "[0.61930134]\n",
      "[0.61895412]\n",
      "[0.61860764]\n",
      "[0.61826192]\n",
      "[0.61791694]\n",
      "[0.61757271]\n",
      "[0.61722923]\n",
      "[0.61688648]\n",
      "[0.61654447]\n",
      "[0.61620321]\n",
      "[0.61586267]\n",
      "[0.61552288]\n",
      "[0.61518381]\n",
      "[0.61484548]\n",
      "[0.61450788]\n",
      "[0.614171]\n",
      "[0.61383485]\n",
      "[0.61349942]\n",
      "[0.61316472]\n",
      "[0.61283073]\n",
      "[0.61249747]\n",
      "[0.61216492]\n",
      "[0.61183309]\n",
      "[0.61150197]\n",
      "[0.61117156]\n",
      "[0.61084186]\n",
      "[0.61051287]\n",
      "[0.61018459]\n",
      "[0.60985701]\n",
      "[0.60953013]\n",
      "[0.60920396]\n",
      "[0.60887849]\n",
      "[0.60855371]\n",
      "[0.60822963]\n",
      "[0.60790625]\n",
      "[0.60758355]\n",
      "[0.60726155]\n",
      "[0.60694024]\n",
      "[0.60661962]\n",
      "[0.60629968]\n",
      "[0.60598043]\n",
      "[0.60566186]\n",
      "[0.60534397]\n",
      "[0.60502676]\n",
      "[0.60471023]\n",
      "[0.60439437]\n",
      "[0.60407919]\n",
      "[0.60376468]\n",
      "[0.60345085]\n",
      "[0.60313768]\n",
      "[0.60282518]\n",
      "[0.60251335]\n",
      "[0.60220218]\n",
      "[0.60189167]\n",
      "[0.60158183]\n",
      "[0.60127264]\n",
      "[0.60096412]\n",
      "[0.60065624]\n",
      "[0.60034903]\n",
      "[0.60004247]\n",
      "[0.59973656]\n",
      "[0.5994313]\n",
      "[0.59912668]\n",
      "[0.59882272]\n",
      "[0.5985194]\n",
      "[0.59821672]\n",
      "[0.59791469]\n",
      "[0.5976133]\n",
      "[0.59731254]\n",
      "[0.59701242]\n",
      "[0.59671294]\n",
      "[0.5964141]\n",
      "[0.59611588]\n",
      "[0.5958183]\n",
      "[0.59552134]\n",
      "[0.59522502]\n",
      "[0.59492932]\n",
      "[0.59463424]\n",
      "[0.59433979]\n",
      "[0.59404596]\n",
      "[0.59375275]\n",
      "[0.59346016]\n",
      "[0.59316819]\n",
      "[0.59287683]\n",
      "[0.59258609]\n",
      "[0.59229595]\n",
      "[0.59200643]\n",
      "[0.59171752]\n",
      "[0.59142922]\n",
      "[0.59114153]\n",
      "[0.59085443]\n",
      "[0.59056795]\n",
      "[0.59028206]\n",
      "[0.58999678]\n",
      "[0.58971209]\n",
      "[0.589428]\n",
      "[0.58914451]\n",
      "[0.58886162]\n",
      "[0.58857931]\n",
      "[0.5882976]\n",
      "[0.58801648]\n",
      "[0.58773595]\n",
      "[0.587456]\n",
      "[0.58717664]\n",
      "[0.58689786]\n",
      "[0.58661967]\n",
      "[0.58634206]\n",
      "[0.58606503]\n",
      "[0.58578858]\n",
      "[0.5855127]\n",
      "[0.5852374]\n",
      "[0.58496268]\n",
      "[0.58468853]\n",
      "[0.58441495]\n",
      "[0.58414194]\n",
      "[0.5838695]\n",
      "[0.58359762]\n",
      "[0.58332631]\n",
      "[0.58305557]\n",
      "[0.58278539]\n",
      "[0.58251577]\n",
      "[0.58224671]\n",
      "[0.58197821]\n",
      "[0.58171027]\n",
      "[0.58144288]\n",
      "[0.58117605]\n",
      "[0.58090977]\n",
      "[0.58064404]\n",
      "[0.58037887]\n",
      "[0.58011424]\n",
      "[0.57985016]\n",
      "[0.57958663]\n",
      "[0.57932364]\n",
      "[0.5790612]\n",
      "[0.5787993]\n",
      "[0.57853794]\n",
      "[0.57827712]\n",
      "[0.57801683]\n",
      "[0.57775709]\n",
      "[0.57749788]\n",
      "[0.57723921]\n",
      "[0.57698106]\n",
      "[0.57672345]\n",
      "[0.57646638]\n",
      "[0.57620983]\n",
      "[0.5759538]\n",
      "[0.57569831]\n",
      "[0.57544334]\n",
      "[0.57518889]\n",
      "[0.57493497]\n",
      "[0.57468156]\n",
      "[0.57442868]\n",
      "[0.57417632]\n",
      "[0.57392447]\n",
      "[0.57367314]\n",
      "[0.57342232]\n",
      "[0.57317202]\n",
      "[0.57292223]\n",
      "[0.57267295]\n",
      "[0.57242418]\n",
      "[0.57217592]\n",
      "[0.57192817]\n",
      "[0.57168092]\n",
      "[0.57143418]\n",
      "[0.57118794]\n",
      "[0.5709422]\n",
      "[0.57069697]\n",
      "[0.57045223]\n",
      "[0.57020799]\n",
      "[0.56996425]\n",
      "[0.56972101]\n",
      "[0.56947826]\n",
      "[0.569236]\n",
      "[0.56899424]\n",
      "[0.56875297]\n",
      "[0.56851218]\n",
      "[0.56827189]\n",
      "[0.56803208]\n",
      "[0.56779276]\n",
      "[0.56755393]\n",
      "[0.56731558]\n",
      "[0.56707771]\n",
      "[0.56684032]\n",
      "[0.56660341]\n",
      "[0.56636698]\n",
      "[0.56613103]\n",
      "[0.56589556]\n",
      "[0.56566056]\n",
      "[0.56542604]\n",
      "[0.56519199]\n",
      "[0.56495841]\n",
      "[0.56472531]\n",
      "[0.56449267]\n",
      "[0.5642605]\n",
      "[0.5640288]\n",
      "[0.56379756]\n",
      "[0.56356679]\n",
      "[0.56333649]\n",
      "[0.56310664]\n",
      "[0.56287726]\n",
      "[0.56264834]\n",
      "[0.56241988]\n",
      "[0.56219187]\n",
      "[0.56196433]\n",
      "[0.56173724]\n",
      "[0.5615106]\n",
      "[0.56128442]\n",
      "[0.56105869]\n",
      "[0.56083341]\n",
      "[0.56060858]\n",
      "[0.5603842]\n",
      "[0.56016027]\n",
      "[0.55993679]\n",
      "[0.55971375]\n",
      "[0.55949116]\n",
      "[0.55926901]\n",
      "[0.5590473]\n",
      "[0.55882604]\n",
      "[0.55860521]\n",
      "[0.55838483]\n",
      "[0.55816488]\n",
      "[0.55794537]\n",
      "[0.5577263]\n",
      "[0.55750766]\n",
      "[0.55728946]\n",
      "[0.55707168]\n",
      "[0.55685435]\n",
      "[0.55663744]\n",
      "[0.55642096]\n",
      "[0.55620491]\n",
      "[0.55598928]\n",
      "[0.55577409]\n",
      "[0.55555932]\n",
      "[0.55534497]\n",
      "[0.55513105]\n",
      "[0.55491755]\n",
      "[0.55470447]\n",
      "[0.55449181]\n",
      "[0.55427957]\n",
      "[0.55406775]\n",
      "[0.55385635]\n",
      "[0.55364536]\n",
      "[0.55343479]\n",
      "[0.55322463]\n",
      "[0.55301489]\n",
      "[0.55280555]\n",
      "[0.55259663]\n",
      "[0.55238812]\n",
      "[0.55218002]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "\n",
    "#we read data from files line by line\n",
    "init = False\n",
    "file = open('logistic_x.txt', 'rb')\n",
    "for row in file:\n",
    "    r = row.decode('utf8').strip().split(' ')\n",
    "    if(init == False):\n",
    "        x_train = np.array([[1], [np.float(r[0])], [np.float(r[len(r)-1])]])\n",
    "        init = True\n",
    "    else:\n",
    "        x_train = np.append(x_train, [[1], [np.float(r[0])], [np.float(r[len(r)-1])]], axis=1);\n",
    "init = False\n",
    "file = open('logistic_y.txt', 'rb')\n",
    "for row in file:\n",
    "    if(init == False):\n",
    "        y_train = np.array([[np.float(row.strip())]])\n",
    "        init = True\n",
    "    else:\n",
    "        y_train = np.append(y_train, [[np.float(row.strip())]], axis=1);\n",
    "#number of training examples\n",
    "m = y_train.shape[1]\n",
    "#init theta\n",
    "theta = np.array(np.zeros((x_train.shape[0], 1)))\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "#we find all indices that make y=1 and y=0\n",
    "pos = np.flatnonzero(y_train == 1)\n",
    "neg = np.flatnonzero(y_train == 0)\n",
    "\n",
    "#plot data points\n",
    "plt.plot(x_train[1, pos], x_train[2, pos], 'ro')\n",
    "plt.plot(x_train[1, neg], x_train[2, neg], 'bo')    \n",
    "    \n",
    "x = 0\n",
    "xT = x_train.T\n",
    "yT = y_train.T\n",
    "preJ = 0\n",
    "while True:\n",
    "    J = 0\n",
    "    x = x + 1;\n",
    "    for i in range(0, m):\n",
    "        #calculate h, error, cost function for 1 training example\n",
    "        h = sigmoid(theta.T.dot(x_train[:,i].T))\n",
    "        error = h.T - yT[i]\n",
    "        tmp = (-1)*yT[i]*np.log(h) - (1-yT[i])*np.log((1-h))\n",
    "        #accumulate cost function\n",
    "        J = J + tmp\n",
    "        nX = np.array([x_train[:,i]]).T\n",
    "        #update theta\n",
    "        theta = theta - 0.000003*(error*nX)\n",
    "    J=J/m\n",
    "    #just print cost function for every 1000steps\n",
    "    if(x == 1000):\n",
    "        x = 0\n",
    "        print(J)\n",
    "    if(preJ == 0):\n",
    "        preJ = J\n",
    "    #condition to stop learning when cost function do not decrease anymore\n",
    "    if((preJ) < (J)):\n",
    "        break\n",
    "    else:\n",
    "        preJ = J\n",
    "#we got theta\n",
    "print(theta)\n",
    "\n",
    "#plot the line that separate data poits\n",
    "plot_x = [np.ndarray.min(x_train[1:]), np.ndarray.max(x_train[1:])]\n",
    "plot_y = np.subtract(np.multiply(-(theta[2][0]/theta[1][0]), plot_x), theta[0][0]/theta[1][0])\n",
    "plt.plot(plot_x, plot_y, 'b-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "\n",
    "init = False\n",
    "file = open('logistic_x.txt', 'rb')\n",
    "for row in file:\n",
    "    r = row.decode('utf8').strip().split(' ')\n",
    "    if(init == False):\n",
    "        x_train = np.array([[1], [np.float(r[0])], [np.float(r[len(r)-1])]])\n",
    "        init = True\n",
    "    else:\n",
    "        x_train = np.append(x_train, [[1], [np.float(r[0])], [np.float(r[len(r)-1])]], axis=1);\n",
    "init = False\n",
    "file = open('logistic_y.txt', 'rb')\n",
    "for row in file:\n",
    "    if(init == False):\n",
    "        y_train = np.array([[np.float(row.strip())]])\n",
    "        init = True\n",
    "    else:\n",
    "        y_train = np.append(y_train, [[np.float(row.strip())]], axis=1);\n",
    "\n",
    "m = y_train.shape[1]\n",
    "theta = np.zeros((x_train.shape[0], 1))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    " return 1/(1+np.exp(-z))\n",
    "\n",
    "pos = np.flatnonzero(y_train == 1)\n",
    "neg = np.flatnonzero(y_train == 0)\n",
    "\n",
    "plt.plot(x_train[1, pos], x_train[2, pos], 'ro')\n",
    "plt.plot(x_train[1, neg], x_train[2, neg], 'bo')  \n",
    "\n",
    "yT = y_train.T\n",
    "xT = x_train.T\n",
    "#iterator 500 steps\n",
    "for x in range(0, 10):\n",
    "    h = sigmoid(theta.T.dot(x_train))\n",
    "    error = h - y_train\n",
    "    tmp = (-1)*y_train*np.log(h) - (1-y_train)*np.log((1-h))\n",
    "    J = np.sum(tmp)/m;\n",
    "    #calculate H\n",
    "    H = (h*(1-h)*(x_train)).dot(x_train.T)/m\n",
    "    #calculate dJ\n",
    "    dJ = np.sum(error*x_train, axis=1)/m\n",
    "    #gradient = H-1.dJ\n",
    "    grad = inv(H).dot(dJ)\n",
    "    #update theta\n",
    "    theta = theta - (np.array([grad])).T\n",
    "    print(J)\n",
    "    \n",
    "print(theta)\n",
    "\n",
    "plot_x = [np.ndarray.min(x_train[1:]), np.ndarray.max(x_train[1:])]\n",
    "plot_y = np.subtract(np.multiply(-(theta[2][0]/theta[1][0]), plot_x), theta[0][0]/theta[1][0])\n",
    "plt.plot(plot_x, plot_y, 'b-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
