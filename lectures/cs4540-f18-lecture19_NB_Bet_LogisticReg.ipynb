{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\vx}{\\vec{x}}\n",
    "\\newcommand{\\I}{\\mathbb{I}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# Lecture 19: Logistic Regression\n",
    "\n",
    "Naveen Kodali and Jacob Abernethy\n",
    "*Date:  Tuesday, November 6, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Problem\n",
    "\n",
    "- We will use **Naive Bayes** to solve the following classification problem:\n",
    "    - **Categorical** feature vector $\\vx = (x_1, x_2, \\dots, x_D)$ with length $D$\n",
    "        - Each feature $x_d \\in \\{0,1\\}$, $\\forall d = 1, \\dots, D$\n",
    "        - Note: you can allow for non-binary features - $x_d \\in \\{0,1, \\ldots M\\}$\n",
    "    - Predict discrete class label $y \\in \\{1, 2, \\dots, C \\}$\n",
    "\n",
    "- For example, in **Spam Mail Classification**,\n",
    "    - Predict whether an email is `SPAM` ($y=1$) or `HAM` ($y=0$)\n",
    "    - Use words / metadata in the email as features\n",
    "    - For simplicity, we can use **bag-of-words** features,\n",
    "        - Assume fixed vocabulary $V$ of size $|V| = D$\n",
    "        - Feature $x_d$, for $d \\in \\{1, 2, \\dots, D \\}$, indicates the existence of $d\\text{th}$ word in the email\n",
    "        - Eg. $x_d = 1$ if $d\\text{th}$ word is in the email; $x_d = 0$ otherwise\n",
    "        - In this case $M=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Independence Assumption and Full model\n",
    "\n",
    "- The essence of Naive Bayes is the **conditionally independence assumption**\n",
    "    $$\n",
    "    P(\\vx | y = c) = \\prod_{d=1}^D P(x_d | y=c)\n",
    "    $$\n",
    "    i.e., given the label, all features are independent.\n",
    "    \n",
    "- The **full generative** model of Naive Bayes is:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    P(y = c ) & = \\pi_c \\quad \\forall\\, c=0,1 \\\\\n",
    "    P(x_d = 1 | y = c ) &= \\theta_{cd} \\quad \\forall\\, d = 1,\\dots,D\n",
    "    \\end{align}\n",
    "    $$\n",
    "- Parameter $\\pi$ and $\\theta$ are learned from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Prediction\n",
    "\n",
    "- Given the independence assumption and full model, for some new data $\\vx^{\\text{new}} = (x_1^{\\text{new}}, \\dots, x_D^{\\text{new}})$ we will classify based on\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(y=c|\\vx = \\vx^{\\text{new}}) \\\\\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(\\vx = \\vx^{\\text{new}} | y=c) P(y=c) \\\\\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(y=c) \\prod \\nolimits_{d=1}^{D} P(x_d = x_d^{\\text{new}} | y=c) \\\\\n",
    "    &=\\boxed{\\underset{c \\in \\{0,1\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\theta_{cd}^{x_d^{\\text{new}}} (1-\\theta_{cd})^{1-x_d^{\\text{new}}}} \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "- So as long as we learned parameter $\\pi$ and $\\theta$, we could classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "\n",
    "- We have alread solved the MLE for the multinomial distribution (categorical variable)! We observed that:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cd} = \\frac{N_{cd}}{N_c}\n",
    "    $$\n",
    "    where\n",
    "    - $N = $ Number of examples in $\\mathcal{D}$\n",
    "    - $N_c = $ Number of examples in class $c$ in $\\mathcal{D}$\n",
    "    - $N_{cd} = $ Number of examples in class $c$ with $x_d = 1$\n",
    "    \n",
    "- Intuitive Interpretation\n",
    "    - The class prior $\\pi$ is obtained from the density of each class $\\{1, \\dots, C\\}$ in $\\mathcal{D}$\n",
    "    - The class-conditional probability $\\theta_{cd}$ is obtained from the density of $x_d \\in \\{0,1\\}$ among all examples in class $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example dataset\n",
    "#### Example No.  |  Color  |  Type  |  Origin  |  Stolen?\n",
    "\n",
    "1 | Red | Sports | Domestic | Yes <br>\n",
    "2 | Red | Sports | Domestic | No <br>\n",
    "3 | Red | Sports | Domestic | Yes <br>\n",
    "4 | Yellow | Sports | Domestic | No <br>\n",
    "5 | Yellow | Sports | Imported | Yes <br>\n",
    "6 | Yellow | SUV | Imported | No <br>\n",
    "7 | Yellow | SUV | Imported | Yes <br>\n",
    "8 | Yellow | SUV | Domestic | No <br>\n",
    "9 | Red | SUV | Imported | No <br>\n",
    "10 | Red | Sports | Imported | Yes \n",
    "\n",
    "### Problem\n",
    "\n",
    "What is MLE of the parameters $\\theta$ and $\\mu$? \n",
    "\n",
    "For these parameters, what is P(Yes | Red Domestic SUV) and (No | Red Domestic SUV)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MAP Estimation for Naive Bayes with Beta Prior\n",
    "\n",
    "In the above example, what if we never see a red car that is stolen (perhaps because we didn't have much data)? What will be $P(\\text{Stolen} | \\text{Red Imported Sports})$? The predicted probability will be 0! This is not desireable, since it would essentially be \"overfitting\" to the data.\n",
    "\n",
    "This is where we want a prior distribution. As we discussed previously, it's best to use a conjugate prior if you can, because the calculations are very convenient. The conjugate distribution to the binomial model is the *beta distribution*, parameterized by $\\alpha, \\beta > 0$:\n",
    "$$P(\\theta | \\alpha, \\beta) := \\frac{\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}$$\n",
    "where the normalization term $B$ is defined in terms of the [gamma function](https://en.wikipedia.org/wiki/Gamma_function), $B(\\alpha, \\beta) := \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum a Posteriori\n",
    "\n",
    "\n",
    "- We have alread solved the MLE for Naive Bayes:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cd} = \\frac{N_{cd}}{N_c}\n",
    "    $$\n",
    "    where $N = $ #examples in the dataset, $N_c = $ #examples in class $c$ in dataset, $N_{cd} = $ #examples in class $c$ with $x_d = 1$\n",
    "    \n",
    "*Problem*: What is the MAP estimate of the parameters $\\theta_{cd}$ for this model, when we assume the prior on every $\\theta_{cd}$ is (independently) distributed according to $\\text{Beta}(\\alpha,\\beta)$?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Answer\n",
    "\n",
    "You get the \"smoothed\" version of the counts:\n",
    "    $$\n",
    "     \\hat{\\theta}_{cd}^{\\text{MAP}(\\alpha,\\beta)} = \\frac{N_{cd} + \\alpha}{N_c + \\alpha + \\beta}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary is to Categorical as Beta is to Dirichlet\n",
    "\n",
    "We want to recall that the binomial model is a distribution on (counts of) *binary* variables. In the example above, all of the features $x_d$ took one of two values, and the class label YES/NO was also binary. In this case, you only need $\\theta_{cd}$ for each class $c \\in \\{0,1\\}$ and feature index $d = 1, \\ldots, D$\n",
    "\n",
    "*BUT* what if the features and classes can take one of more than two classes? In this case, we would assume our features and classes are *categorical* variables, and we would use the *multinomial* distribution to model them. If $X$ is a categorical variable with parameter vector $q$, then $P(X = j|q) = q_j$.\n",
    "\n",
    "The Beta prior is the \"good\" prior for the binomial distribution. For the categorical, it's the *Dirichlet* distribution. Let $q \\in \\Delta_K$ be some probability distribution on $K$ classes. Given parameters $\\alpha_1, \\ldots, \\alpha_K$, the Dirichlet distribution for $\\vec \\alpha$ has PDF:\n",
    "$$P_{\\vec \\alpha}(q) := \\frac{1}{B(\\vec \\alpha)} \\prod_{i=1}^K q_i^{\\alpha_i - 1}$$\n",
    "where $B(\\vec \\alpha) := \\frac{\\prod_{i=1}^K \\Gamma(\\alpha_i)}{\\Gamma(\\sum_{i=1}^K \\alpha_i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Part 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our models so far:\n",
    "\n",
    "+ Linear regression\n",
    "    + Tries to find a model to predict $y$ from $x \\in \\R^d$, with parameters $\\theta \\in \\R^d$\n",
    "    + The variable $y$ is a *real* number\n",
    "    + Model assumes that $y$ is Gaussian with mean $x^\\top \\theta$ and variance $\\sigma^2$\n",
    "    + Model does **not** treat $x$ as a random variable\n",
    "+ Naive Bayes\n",
    "    + Tries to find a model to predict $y$ from $x$, with parameters $\\theta, \\mu$\n",
    "    + (Typically) the variable $y$ is a *categorical* variable\n",
    "    + (Typically) the coordinates of $x$ are *categorical* variables\n",
    "    + Model **does** treat $x$ as a random variable\n",
    "    + The model makes the (somewhat unusual) assumption that features $x = (x_1, \\ldots, x_D)$ are *independent once we condition on $y$*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## New Model: Logistic Regression\n",
    "\n",
    "+ Logistic regression\n",
    "    + Tries to find a model to predict $y$ from $x \\in \\R^d$, with parameters $\\theta \\in \\R^d$\n",
    "    + Model assumes that variable $y$ is a *categorical* variable with parameters $q$\n",
    "    + The prediction for $y$ is a probability distribution $q = P(y=1|x,\\theta), \\ldots, P(y=K|x,\\theta)$\n",
    "    + Model does **not** treat $x$ as a random variable\n",
    "    + Despite the name, LR is really a *classification* algorithm, not a regression alg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The likelihood model for Logistic Regression\n",
    "\n",
    "In this lecture, let us assume that the categorical label $y$ is *binary*, indeed $y \\in \\{-1,1\\}$, just to make things easier. But it is not hard to generalize this to multiclass (categorical > 2) settings.\n",
    "\n",
    "Given input $x$ and parameters $\\theta$, the prediction for $y$ can be viewed as a probability $q$ where\n",
    "$$q := P(y = 1 | x, \\theta) = \\frac{1}{1 + \\exp(-x^\\top \\theta)} = \\frac{\\exp(x^\\top \\theta)}{1 + \\exp(x^\\top \\theta)}.$$\n",
    "\n",
    "What's going on here? The problem is that $\\theta^\\top x$ is not guaranteed to be in $[0,1]$. We need to \"squash\" the real number $\\theta^\\top x$ into the probability space $[0,1]$. The standard way to do this is with the *sigmoid* function $\\sigma(s) = \\frac{1}{1 + \\exp(-s)}$.\n",
    "\n",
    "A nice property of the sigmoid: $\\sigma(-s) = (1-\\sigma(s))$. This is used a lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Log odds ratio\n",
    "\n",
    "It's often interesting to define what we call the *log-odds ratio* of a probability $p$ as $\\log \\frac{p}{1-p}$.\n",
    "\n",
    "\n",
    "With this in mind, we can also state this the Logistic Regression model as follows: the *log-odds ratio* is linear in $x$. That is, we have\n",
    "$$\\log \\frac{P(y = 1 | x, \\theta)}{P(y = -1 | x, \\theta)} = \\theta^\\top x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Challenge for LR: No closed form MLE!\n",
    "\n",
    "The maximum likelihood estimator for the logistic regression is the following:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\theta^{\\text{MLE}} & := & \\arg\\max_{\\theta \\in \\R^d} \\sum_{i=1}^n \\log P(y_i | x_i, \\theta) \\\\\n",
    "& = & \\arg\\max_{\\theta \\in \\R^d} \\sum_{i=1}^n \\log \\left(\\sigma(x_i^\\top \\theta)^{\\mathbb{I}(y_i = 1)}(1-\\sigma(x_i^\\top \\theta))^{\\mathbb{I}(y_i = -1)}\\right) \\\\\n",
    "& = & \\arg\\max_{\\theta \\in \\R^d} \\sum_{i=1}^n \\log \\sigma(y_i(x_i^\\top \\theta))\n",
    "\\end{eqnarray*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we define $h(x) = \\sigma(x^T\\theta)$, with some algebra, we can alternatively write down the loss for logistic regression as:\n",
    "$$L(\\theta) = \\sum_{i=1}^{m} (−y_i \\log(h(x_i)) − (1−y_i) \\log(1−h(x_i)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "Derive the gradient of the loss function above and what would the GD/SGD steps look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Gradient matrix form: $\\nabla_{\\theta} L(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (h(x_i)−y_i) x_i$\n",
    "\n",
    "SGD Step co-ord wise: $\\theta_j = \\theta_j − \\alpha(h(x_i)−y_i)x_i^j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "Derive the Hessian of the loss function above and what would the Newton's method step look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ h(x_i)(1−h(x_i)) x_i x_i^T \\right]$\n",
    "    \n",
    "Newton Step: $\\theta = \\theta − H^{−1} \\nabla_\\theta L(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-06 14:57:55--  https://raw.githubusercontent.com/cs4540-f18/cs4540-f18.github.io/master/lectures/logistic_x.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2399 (2.3K) [text/plain]\n",
      "Saving to: ‘logistic_x.txt.1’\n",
      "\n",
      "logistic_x.txt.1    100%[===================>]   2.34K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-11-06 14:57:56 (21.6 MB/s) - ‘logistic_x.txt.1’ saved [2399/2399]\n",
      "\n",
      "--2018-11-06 14:57:56--  https://raw.githubusercontent.com/cs4540-f18/cs4540-f18.github.io/master/lectures/logistic_y.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1119 (1.1K) [text/plain]\n",
      "Saving to: ‘logistic_y.txt.1’\n",
      "\n",
      "logistic_y.txt.1    100%[===================>]   1.09K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-11-06 14:57:56 (27.4 MB/s) - ‘logistic_y.txt.1’ saved [1119/1119]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/cs4540-f18/cs4540-f18.github.io/master/lectures/logistic_x.txt\n",
    "!wget https://raw.githubusercontent.com/cs4540-f18/cs4540-f18.github.io/master/lectures/logistic_y.txt\n",
    "###If this doesnt work in Colab, just clone class repo to your computer and open the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55.5 41.  53.5 46.  41.  51.5 51.  42.  53.5 57.5 42.5 41.  46.  46.\n",
      " 49.5 41.  48.5 51.5 44.5 44.  33.  33.5 31.5 33.  42.  30.  61.  49.\n",
      " 26.5 34.  42.  29.5 39.5 51.5 41.5 42.5 35.  38.5 32.  46.  36.5 36.5\n",
      " 24.  19.  34.5 37.5 35.5 37.  21.5 35.5 26.5 26.5 18.5 40.  32.5 39.\n",
      " 43.  22.  36.  31.  38.5 40.  37.5 24.5 30.  33.  56.5 41.  49.5 34.5\n",
      " 32.5 36.  27.  41.  29.5 20.  38.  18.5 16.  33.5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Setting up ---- dont worry about this part\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "\n",
    "init = False\n",
    "file = open('logistic_x.txt', 'rb')\n",
    "for row in file:\n",
    "    r = row.decode('utf8').strip().split(' ')\n",
    "    if(init == False):\n",
    "        x_train = np.array([[1], [np.float(r[0])], [np.float(r[len(r)-1])]])\n",
    "        init = True\n",
    "    else:\n",
    "        x_train = np.append(x_train, [[1], [np.float(r[0])], [np.float(r[len(r)-1])]], axis=1);\n",
    "init = False\n",
    "file = open('logistic_y.txt', 'rb')\n",
    "for row in file:\n",
    "    if(init == False):\n",
    "        y_train = np.array([[np.float(row.strip())]])\n",
    "        init = True\n",
    "    else:\n",
    "        y_train = np.append(y_train, [[np.float(row.strip())]], axis=1);\n",
    "\n",
    "m = y_train.shape[1]\n",
    "theta = np.zeros((x_train.shape[0], 1))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    " return 1/(1+np.exp(-z))\n",
    "\n",
    "pos = np.flatnonzero(y_train == 1)\n",
    "neg = np.flatnonzero(y_train == 0)\n",
    "\n",
    "plt.plot(x_train[1, pos], x_train[2, pos], 'ro')\n",
    "plt.plot(x_train[1, neg], x_train[2, neg], 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Training with Newton's method which you will implement!!!!\n",
    "yT = y_train.T\n",
    "xT = x_train.T\n",
    "#iterator 500 steps\n",
    "for x in range(0, 10):\n",
    "    h = sigmoid(theta.T.dot(x_train))\n",
    "    error = h - y_train\n",
    "    tmp = (-1)*y_train*np.log(h) - (1-y_train)*np.log((1-h))\n",
    "    J = np.sum(tmp)/m;\n",
    "    #calculate H\n",
    "    #Write your line of code here\n",
    "    \n",
    "    #calculate gradient\n",
    "    #Write your line of code here\n",
    "    \n",
    "    #Update theta\n",
    "    #Write your lines of code here\n",
    "    print(J)\n",
    "    \n",
    "print(theta)\n",
    "\n",
    "plot_x = [np.ndarray.min(x_train[1:]), np.ndarray.max(x_train[1:])]\n",
    "plot_y = np.subtract(np.multiply(-(theta[2][0]/theta[1][0]), plot_x), theta[0][0]/theta[1][0])\n",
    "plt.plot(plot_x, plot_y, 'b-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "\n",
    "#we read data from files line by line ------ dont worry about this part\n",
    "init = False\n",
    "file = open('logistic_x.txt', 'rb')\n",
    "for row in file:\n",
    "    r = row.decode('utf8').strip().split(' ')\n",
    "    if(init == False):\n",
    "        x_train = np.array([[1], [np.float(r[0])], [np.float(r[len(r)-1])]])\n",
    "        init = True\n",
    "    else:\n",
    "        x_train = np.append(x_train, [[1], [np.float(r[0])], [np.float(r[len(r)-1])]], axis=1);\n",
    "init = False\n",
    "file = open('logistic_y.txt', 'rb')\n",
    "for row in file:\n",
    "    if(init == False):\n",
    "        y_train = np.array([[np.float(row.strip())]])\n",
    "        init = True\n",
    "    else:\n",
    "        y_train = np.append(y_train, [[np.float(row.strip())]], axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111609590>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#number of training examples ------- dont worry about this part\n",
    "m = y_train.shape[1]\n",
    "#init theta\n",
    "theta = np.array(np.zeros((x_train.shape[0], 1)))\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "#we find all indices that make y=1 and y=0\n",
    "pos = np.flatnonzero(y_train == 1)\n",
    "neg = np.flatnonzero(y_train == 0)\n",
    "\n",
    "#plot data points\n",
    "plt.plot(x_train[1, pos], x_train[2, pos], 'ro')\n",
    "plt.plot(x_train[1, neg], x_train[2, neg], 'bo')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66562014]\n",
      "[0.66518165]\n",
      "[0.66473473]\n",
      "[0.66428829]\n",
      "[0.66384282]\n",
      "[0.66339833]\n",
      "[0.66295481]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-17fdac0bf138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0myT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0myT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#accumulate cost function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###This is training part which uses SGD/GD ----- dont run this in class but later compare it against Newton's \n",
    "###method which you will implement below.\n",
    "x = 0\n",
    "xT = x_train.T\n",
    "yT = y_train.T\n",
    "preJ = 0\n",
    "while True:\n",
    "    J = 0\n",
    "    x = x + 1;\n",
    "    for i in range(0, m):\n",
    "        #calculate h, error, cost function for 1 training example\n",
    "        h = sigmoid(theta.T.dot(x_train[:,i].T))\n",
    "        error = h.T - yT[i]\n",
    "        tmp = (-1)*yT[i]*np.log(h) - (1-yT[i])*np.log((1-h))\n",
    "        #accumulate cost function\n",
    "        J = J + tmp\n",
    "        nX = np.array([x_train[:,i]]).T\n",
    "        #update theta\n",
    "        theta = theta - 0.000003*(error*nX)\n",
    "    J=J/m\n",
    "    #just print cost function for every 1000steps\n",
    "    if(x == 1000):\n",
    "        x = 0\n",
    "        print(J)\n",
    "    if(preJ == 0):\n",
    "        preJ = J\n",
    "    #condition to stop learning when cost function do not decrease anymore\n",
    "    if((preJ) < (J)):\n",
    "        break\n",
    "    else:\n",
    "        preJ = J\n",
    "#we got theta\n",
    "print(theta)\n",
    "\n",
    "#plot the line that separate data poits\n",
    "plot_x = [np.ndarray.min(x_train[1:]), np.ndarray.max(x_train[1:])]\n",
    "plot_y = np.subtract(np.multiply(-(theta[2][0]/theta[1][0]), plot_x), theta[0][0]/theta[1][0])\n",
    "plt.plot(plot_x, plot_y, 'b-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Answers\n",
    "## H = (h*(1-h)*(x_train)).dot(x_train.T)/m\n",
    "\n",
    "## dJ = np.sum(error*x_train, axis=1)/m\n",
    "\n",
    "###gradient = H-1.dJ\n",
    "###grad = inv(H).dot(dJ)\n",
    "###update theta\n",
    "###theta = theta - (np.array([grad])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not impressed, go home and try gradient descent!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
