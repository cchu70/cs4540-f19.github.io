{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LaTeX \\text{ commands here}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\im}{\\text{im}\\,}\n",
    "\\newcommand{\\norm}[1]{||#1||}\n",
    "\\newcommand{\\inner}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\span}{\\mathrm{span}}\n",
    "\\newcommand{\\proj}{\\mathrm{proj}}\n",
    "\\newcommand{\\OPT}{\\mathrm{OPT}}\n",
    "\\newcommand{\\vx}{\\vec{x}}\n",
    "\\newcommand{\\I}{\\mathbb{I}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<hr style=\"border: 5px solid black\">\n",
    "\n",
    "**Georgia Tech, CS 4540**\n",
    "\n",
    "# Lecture 18: Naive Bayes Algorithm, and the Beta-Binomial Model\n",
    "\n",
    "Naveen Kodali and Jacob Abernethy\n",
    "*Date:  Thursday, November 1, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "- Basics\n",
    "    - Review of MAP (*maximum a posteriori*) estimation\n",
    "    - MAP estimation for linear regression\n",
    "- Naive Bayes Classifier\n",
    "    - Independence Assumption\n",
    "    - MLE Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood: Simple Linear Regression\n",
    "\n",
    "One of the most basic techniques for trying to estimate a real number $y$ given an observed vector $x \\in \\R^d$ is known as *linear regression*, aka *least squares*. The key idea is to assume that the data $x_1, \\ldots, x_n$ are fixed vectors, and the labels $y_1, \\ldots, y_n$ are *independent* random real numbers whose probabiliy distribution is *guassian*. For some parameter $\\theta \\in \\R^d$ we have\n",
    "$$P( y_i | x_i, \\theta) = \\mathcal{N}(y_i | x_i^\\top \\theta, \\sigma^2)= \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^\\top \\theta)^2}{2\\sigma^2}\\right)$$\n",
    "**Problem**: What is the MLE for $\\theta$? (You don't necessarily have to find a closed-form solution)\n",
    "\n",
    "Useful fact: by independence we have $P(y_1, \\ldots, y_n | x_1, \\ldots, x_n, \\theta) = P(y_1 | x_1, \\theta) \\cdots P(y_n | x_n, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Want:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg\\max_\\theta & \\; \\sum_{i=1}^n \\log\\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta^\\top x_i)^2}{2\\sigma^2}\\right)\\right) \\\\\n",
    "= \\arg\\max_\\theta & \\; \\sum_{i=1}^n \\log \\exp\\left(-\\frac{(y_i - \\theta^\\top x_i)^2}{2\\sigma^2}\\right)\n",
    "\\\\\n",
    "= \\arg\\min_\\theta & \\; \\sum_{i=1}^n (y_i - \\theta^\\top x_i)^2\\\\\n",
    "\\nabla = 0 \\implies & \\;  \\sum_{i=1}^n (y_i - \\theta^\\top x_i) x_i^\\top = 0\\\\\n",
    "\\implies & \\;  \\theta (X^\\top X) = X^\\top Y\n",
    "\\end{align*}\n",
    "$$\n",
    "where $X$ is the matrix whose rows are $x_1^\\top, \\ldots, x_n^\\top$ and $Y$ is the vector with entries $y_1, \\ldots, y_n$. Now we are solving a linear system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum A Posterior estimation\n",
    "\n",
    "Previously, we have been looking at the Max. Likelihood estimator. A more advanced estimator involves a *prior* on the parameters. This means that we \"prefer\", a priori, certain parameters over others. In other words, we think some parameters are more likely to be true than others. You describe your \"preference\" for some parameters using a prior distribution $P(\\theta)$. Given data $x_1, \\ldots, x_n$, the Maximum A Posterior (MAP) estimate for $\\theta$ is then\n",
    "$$\\arg\\max_\\theta P(\\theta) \\prod_{i=1}^n P(x_i | \\theta) \\quad \\text{ equiv. to } \\quad \n",
    "\\arg\\max_\\theta \\log P(\\theta) +  \\sum_{i=1}^n \\log P(x_i | \\theta) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "For the linear regression model above, we can add a prior to the parameter vector $\\theta$. The standard prior is the so-called \"multivariate gaussian\" with a single *fixed* parameter $\\nu$, i.e.\n",
    "$$P(\\theta) = \\frac{1}{(2\\pi \\nu^2)^{d/2}} \\exp\\left(\\frac{-\\|\\theta\\|^2}{2\\nu^2}\\right)$$\n",
    "\n",
    "The likelihood function is still the same:\n",
    "$P( y_i | x_i, \\theta) =\n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^\\top \\theta)^2}{2\\sigma^2}\\right)$.\n",
    "\n",
    "**Problem A**: What is the MAP estimate for this model *before* we have data?\n",
    "$$\\arg\\max_\\theta \\log P(\\theta)$$\n",
    "\n",
    "**Problem B**: What is the MAP estimate for this model *after* we have data?\n",
    "$$\\arg\\max_\\theta \\log P(\\theta) +  \\sum_{i=1}^n \\log P(y_i | x_i, \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Observation\n",
    "\n",
    "* We use the term *posterior distribution* to refer to the probability of the parameters $\\theta$ given the dataset $(x_1, y_1), \\ldots, (x_n,y_n)$. You can see from Bayes rule that\n",
    "$$P(\\theta | data) \\propto P(data | \\theta) P(\\theta) = P(\\theta) \\prod_i P(y_i | x_i, \\theta)$$\n",
    "* If you work this out for the ridge regression model discussed above, you find that\n",
    "$$P(\\theta | data) \\propto \\exp\\left( - \\frac{1}{2} (\\theta - \\mu)^\\top \\Lambda (\\theta-\\mu)\\right)$$\n",
    "where $\\Lambda = I + X^\\top X$ and $\\mu = \\Lambda^{-1} X^\\top Y$. In other words, the distribution on $\\theta$ given the data is now a multivariate gaussian $\\mathcal{N}(\\mu, \\Lambda^{-1})$ with mean $\\mu$ and covariance matrix $\\Lambda^{-1}$.\n",
    "* When the posterior distribution is *in the same distribution family* as the prior distirbution, we say the prior is *conjugate to* the likelihood. But this is very lucky when this happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Classification\n",
    "\n",
    "- Given some data $\\{x_1,...,x_n\\}$ and their labels $\\{y_1,...,y_n\\} \\in \\{1,...,K\\}$. The goal of classification is to find a function $f: \\mathcal{X} \\to P(Y=k|X)$ that fits this data and also performs well on unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Problem\n",
    "\n",
    "- We will use **Naive Bayes** to solve the following classification problem:\n",
    "    - **Categorical** feature vector $\\vx = (x_1, x_2, \\dots, x_D)$ with length $D$\n",
    "        - Each feature $x_d \\in \\{0,1\\}$, $\\forall d = 1, \\dots, D$\n",
    "        - Note: you can allow for non-binary features - $x_d \\in \\{0,1, \\ldots M\\}$\n",
    "    - Predict discrete class label $y \\in \\{1, 2, \\dots, C \\}$\n",
    "\n",
    "- For example, in **Spam Mail Classification**,\n",
    "    - Predict whether an email is `SPAM` ($y=1$) or `HAM` ($y=0$)\n",
    "    - Use words / metadata in the email as features\n",
    "    - For simplicity, we can use **bag-of-words** features,\n",
    "        - Assume fixed vocabulary $V$ of size $|V| = D$\n",
    "        - Feature $x_d$, for $d \\in \\{1, 2, \\dots, D \\}$, indicates the existence of $d\\text{th}$ word in the email\n",
    "        - Eg. $x_d = 1$ if $d\\text{th}$ word is in the email; $x_d = 0$ otherwise\n",
    "        - In this case $M=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Independence Assumption and Full model\n",
    "\n",
    "- The essence of Naive Bayes is the **conditionally independence assumption**\n",
    "    $$\n",
    "    P(\\vx | y = c) = \\prod_{d=1}^D P(x_d | y=c)\n",
    "    $$\n",
    "    i.e., given the label, all features are independent.\n",
    "    \n",
    "- The **full generative** model of Naive Bayes is:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    P(y = c ) & = \\pi_c \\quad \\forall\\, c=0,1 \\\\\n",
    "    P(x_d = 1 | y = c ) &= \\theta_{cd} \\quad \\forall\\, d = 1,\\dots,D\n",
    "    \\end{align}\n",
    "    $$\n",
    "- Parameter $\\pi$ and $\\theta$ are learned from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ">**Remark**\n",
    "> - **NOTE** in definition and derivation of this lecture, we assume a more general case $x_d \\in \\{1, \\dots ,M \\}$ of which $M>2$. But in spam email classification and the derivation in textbook, binary feature, i.e. $M=2$, is used. So don't get confused!\n",
    "\n",
    "> - When $M=2$, $x_d | y=c$ is also Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Prediction\n",
    "\n",
    "- Given the independence assumption and full model, for some new data $\\vx^{\\text{new}} = (x_1^{\\text{new}}, \\dots, x_D^{\\text{new}})$ we will classify based on\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    y\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(y=c|\\vx = \\vx^{\\text{new}}) \\\\\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(\\vx = \\vx^{\\text{new}} | y=c) P(y=c) \\\\\n",
    "    &=\\underset{c \\in \\{0,1\\}}{\\arg \\max} P(y=c) \\prod \\nolimits_{d=1}^{D} P(x_d = x_d^{\\text{new}} | y=c) \\\\\n",
    "    &=\\boxed{\\underset{c \\in \\{0,1\\}}{\\arg \\max} \\pi_c \\prod \\nolimits_{d=1}^{D} \\theta_{cd}^{x_d^{\\text{new}}} (1-\\theta_{cd})^{1-x_d^{\\text{new}}}} \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "- So as long as we learned parameter $\\pi$ and $\\theta$, we could classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> **Remark**\n",
    "\n",
    "> - Indicator function\n",
    "    $$\n",
    "    \\mathbb{I}(m=x_d^{\\text{new}}) = \n",
    "    \\begin{cases}\n",
    "    1 & \\text{ if } m=x_d^{\\text{new}}\\\\ \n",
    "    0 & \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "> - In inner product $\\prod \\nolimits_{m=1}^{M} \\theta_{cdm}^{\\mathbb{I}(m=x_d^{\\text{new}})}$, only $\\theta_{cdx_d^{\\text{new}}}$ is multiplied and all the other multipliers are 1 due to the power of indicator function.\n",
    "    \n",
    "> - One thing to note is that the above classification criterion is the product of a series numbers smaller than 1 which will generate a rather small number. A better way is to take **logarithm** to transform product into summation and then compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Parameter Estimation\n",
    "\n",
    "- **Goal:** Given training data $\\mathcal{D} = \\{ (\\vec{x}_1, y_1), \\dots, (\\vec{x}_N, y_N) \\}$, estimate **class-conditional probabilities** $\\theta$ and **class priors** $\\pi$.\n",
    "\n",
    "\n",
    "- We will discuss the **MLE** and **MAP** parameter estimates.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "- The **likelihood** for a single data case $(\\vec{x}_n, y_n=c)$ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    & P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    &= P(y_n) \\prod \\nolimits_{d=1}^D P(x_{nd}|y_n) \\\\\n",
    "    &= \\prod \\nolimits_{c=1}^C P(y_n=c)^{\\I(y_n=c)} \\cdot \\prod \\nolimits_{c=1}^C \\prod \\nolimits_{d=1}^D \\prod \\nolimits_{m=1}^M P(x_{nd}=m|y_n=c)^{\\I(x_{nd}=m) \\I(y_n=c)}\\\\\n",
    "    &= \\prod \\nolimits_{c=1}^C \\pi_c^{\\I(y_n=c)} \\cdot \\prod \\nolimits_{c=1}^C \\prod \\nolimits_{d=1}^D \\prod \\nolimits_{m=1}^M \\theta_{cdm}^{\\I(x_{nd}=m) \\I(y_n=c)}\\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "- Therefore, the **log-likelihood** is\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    & \\log P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    & = \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c + \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{d=1}^D \\sum \\nolimits_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c) \\log \\theta_{cdm}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    \n",
    "- The **log-likelihood** for all training data $\\mathcal{D} = \\{ (\\vec{x}_n, y_n) \\}_{n=1}^N $ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    & \\log P(\\mathcal{D}| \\pi, \\theta)\\\\\n",
    "    &= \\log \\prod \\nolimits_{n=1}^N P((\\vec{x}_n, y_n) | \\pi, \\theta) = \\sum \\nolimits_{n=1}^N \\log P((\\vec{x}_n, y_n) | \\pi, \\theta) \\\\\n",
    "    &= \\boxed{\\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\I(y_n=c) \\log \\pi_c + \\sum \\nolimits_{n=1}^N \\sum \\nolimits_{c=1}^C \\sum \\nolimits_{d=1}^D \\sum \\nolimits_{m=1}^M \\I(x_{nd}=m) \\I(y_n=c) \\log \\theta_{cdm}}\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum Likelihood\n",
    "\n",
    "\n",
    "- We have alread solved the MLE for the multinomial distribution (categorical variable)! We observed that:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cd} = \\frac{N_{cd}}{N_c}\n",
    "    $$\n",
    "    where\n",
    "    - $N = $ Number of examples in $\\mathcal{D}$\n",
    "    - $N_c = $ Number of examples in class $c$ in $\\mathcal{D}$\n",
    "    - $N_{cd} = $ Number of examples in class $c$ with $x_d = 1$\n",
    "    \n",
    "- Intuitive Interpretation\n",
    "    - The class prior $\\pi$ is obtained from the density of each class $\\{1, \\dots, C\\}$ in $\\mathcal{D}$\n",
    "    - The class-conditional probability $\\theta_{cd}$ is obtained from the density of $x_d \\in \\{0,1\\}$ among all examples in class $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example No.  |  Color  |  Type  |  Origin  |  Stolen?\n",
    "\n",
    "1 | Red | Sports | Domestic | Yes <br>\n",
    "2 | Red | Sports | Domestic | No <br>\n",
    "3 | Red | Sports | Domestic | Yes <br>\n",
    "4 | Yellow | Sports | Domestic | No <br>\n",
    "5 | Yellow | Sports | Imported | Yes <br>\n",
    "6 | Yellow | SUV | Imported | No <br>\n",
    "7 | Yellow | SUV | Imported | Yes <br>\n",
    "8 | Yellow | SUV | Domestic | No <br>\n",
    "9 | Red | SUV | Imported | No <br>\n",
    "10 | Red | Sports | Imported | Yes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question: \n",
    "We want to classify a Red Domestic SUV. Note there is no example of a Red Domestic SUV in our data\n",
    "set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 4.a (Estimation of parameters)\n",
    "\n",
    "We need <br>\n",
    "P(Red|Yes), P(SUV|Yes), P(Domestic|Yes) <br>\n",
    "P(Red|No) , P(SUV|No), and P(Domestic|No) <br>\n",
    "P(Yes) and P(No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 4.b (Classification)\n",
    "\n",
    "Compute P(Yes | Red Domestic SUV) and (No | Red Domestic SUV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MAP Estimation for Naive Bayes with Beta Prior\n",
    "\n",
    "In the above example, what if we never see a red car that is stolen (perhaps because we didn't have much data)? What will be $P(\\text{Stolen} | \\text{Red Imported Sports})$? The predicted probability will be 0! This is not desireable, since it would essentially be \"overfitting\" to the data.\n",
    "\n",
    "This is where we want a prior distribution. As we discussed previously, it's best to use a conjugate prior if you can, because the calculations are very convenient. The conjugate distribution to the binomial model is the *beta distribution*, parameterized by $\\alpha, \\beta > 0$:\n",
    "$$P(\\theta | \\alpha, \\beta) := \\frac{\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}$$\n",
    "where the normalization term $B$ is defined in terms of the [gamma function](https://en.wikipedia.org/wiki/Gamma_function), $B(\\alpha, \\beta) := \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes:  Maximum a Posteriori\n",
    "\n",
    "\n",
    "- We have alread solved the MLE for Naive Bayes:\n",
    "    $$\n",
    "    \\hat{\\pi}_c = \\frac{N_c}{N} \\quad \\hat{\\theta}_{cd} = \\frac{N_{cd}}{N_c}\n",
    "    $$\n",
    "    where $N = $ #examples in the dataset, $N_c = $ #examples in class $c$ in dataset, $N_{cd} = $ #examples in class $c$ with $x_d = 1$\n",
    "    \n",
    "*Problem*: What is the MAP estimate of the parameters $\\theta_{cd}$ for this model, when we assume the prior on every $\\theta_{cd}$ is (independently) distributed according to $\\text{Beta}(\\alpha,\\beta)$?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Answer\n",
    "\n",
    "You get the \"smoothed\" version of the counts:\n",
    "    $$\n",
    "     \\hat{\\theta}_{cd}^{\\text{MAP}(\\alpha,\\beta)} = \\frac{N_{cd} + \\alpha}{N_c + \\alpha + \\beta}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
